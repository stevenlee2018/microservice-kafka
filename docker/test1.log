Attaching to mskafka_postgres_1, mskafka_zookeeper_1, mskafka_kafka_1, mskafka_shipping_1, mskafka_order_1, mskafka_invoicing_1, mskafka_apache_1
[36mpostgres_1   |[0m The files belonging to this database system will be owned by user "postgres".
[36mpostgres_1   |[0m This user must also own the server process.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m The database cluster will be initialized with locale "en_US.utf8".
[36mpostgres_1   |[0m The default database encoding has accordingly been set to "UTF8".
[36mpostgres_1   |[0m The default text search configuration will be set to "english".
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m Data page checksums are disabled.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m fixing permissions on existing directory /var/lib/postgresql/data ... ok
[36mpostgres_1   |[0m creating subdirectories ... ok
[36mpostgres_1   |[0m selecting default max_connections ... 100
[36mpostgres_1   |[0m selecting default shared_buffers ... 128MB
[36mpostgres_1   |[0m selecting dynamic shared memory implementation ... posix
[36mpostgres_1   |[0m creating configuration files ... ok
[33mzookeeper_1  |[0m JMX enabled by default
[33mzookeeper_1  |[0m Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,127 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,194 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,195 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,221 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,310 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,469 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,519 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,519 [myid:] - INFO  [main:ZooKeeperServerMain@95] - Starting server
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,568 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,586 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=c9e621252f7c
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,586 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.7.0_65
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,586 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,586 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,586 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/opt/zookeeper-3.4.6/bin/../build/classes:/opt/zookeeper-3.4.6/bin/../build/lib/*.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/opt/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/opt/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/opt/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/opt/zookeeper-3.4.6/bin/../conf:
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,590 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,590 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,611 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=4.15.0-43-generic
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=root
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/root
[32mkafka_1      |[0m waiting for kafka to be ready
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,612 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/opt/zookeeper-3.4.6
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,626 [myid:] - INFO  [main:ZooKeeperServer@755] - tickTime set to 2000
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,626 [myid:] - INFO  [main:ZooKeeperServer@764] - minSessionTimeout set to -1
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,626 [myid:] - INFO  [main:ZooKeeperServer@773] - maxSessionTimeout set to -1
[32mkafka_1      |[0m [Configuring] 'advertised.port' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m Excluding KAFKA_HOME from broker config
[32mkafka_1      |[0m [Configuring] 'advertised.host.name' in '/opt/kafka/config/server.properties'
[33mzookeeper_1  |[0m 2018-12-30 04:02:13,767 [myid:] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181
[32mkafka_1      |[0m [Configuring] 'port' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m [Configuring] 'broker.id' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m Excluding KAFKA_VERSION from broker config
[32mkafka_1      |[0m [Configuring] 'zookeeper.connect' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m [Configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
[36;1mapache_1     |[0m [Sun Dec 30 04:02:17.045692 2018] [proxy_html:notice] [pid 8:tid 140056020563840] AH01425: I18n support in mod_proxy_html requires mod_xml2enc. Without it, non-ASCII characters in proxied pages are likely to display incorrectly.
[36;1mapache_1     |[0m AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.22.0.8. Set the 'ServerName' directive globally to suppress this message
[36mpostgres_1   |[0m running bootstrap script ... ok
[32mkafka_1      |[0m [2018-12-30 04:02:20,712] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[36mpostgres_1   |[0m performing post-bootstrap initialization ... ok
[32mkafka_1      |[0m waiting for kafka to be ready
[32mkafka_1      |[0m [2018-12-30 04:02:24,014] INFO starting (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-30 04:02:24,020] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[36mpostgres_1   |[0m syncing data to disk ... ok
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m WARNING: enabling "trust" authentication for local connections
[36mpostgres_1   |[0m You can change this by editing pg_hba.conf or using the option -A, or
[36mpostgres_1   |[0m --auth-local and --auth-host, the next time you run initdb.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m Success. You can now start the database server using:
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m     pg_ctl -D /var/lib/postgresql/data -l logfile start
[36mpostgres_1   |[0m 
[32mkafka_1      |[0m [2018-12-30 04:02:24,161] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[36mpostgres_1   |[0m waiting for server to start....LOG:  could not bind IPv6 socket: Cannot assign requested address
[36mpostgres_1   |[0m HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
[32mkafka_1      |[0m [2018-12-30 04:02:24,196] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,198] INFO Client environment:host.name=a1248bd306d5 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,199] INFO Client environment:java.version=1.8.0_171 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,199] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,200] INFO Client environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,202] INFO Client environment:java.class.path=/opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-lang3-3.5.jar:/opt/kafka/bin/../libs/connect-api-2.0.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.0.0.jar:/opt/kafka/bin/../libs/connect-file-2.0.0.jar:/opt/kafka/bin/../libs/connect-json-2.0.0.jar:/opt/kafka/bin/../libs/connect-runtime-2.0.0.jar:/opt/kafka/bin/../libs/connect-transforms-2.0.0.jar:/opt/kafka/bin/../libs/guava-20.0.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0-b42.jar:/opt/kafka/bin/../libs/jackson-annotations-2.9.6.jar:/opt/kafka/bin/../libs/jackson-core-2.9.6.jar:/opt/kafka/bin/../libs/jackson-databind-2.9.6.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.9.6.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.9.6.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.9.6.jar:/opt/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/opt/kafka/bin/../libs/javax.annotation-api-1.2.jar:/opt/kafka/bin/../libs/javax.inject-1.jar:/opt/kafka/bin/../libs/javax.inject-2.5.0-b42.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.27.jar:/opt/kafka/bin/../libs/jersey-common-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.27.jar:/opt/kafka/bin/../libs/jersey-hk2-2.27.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.27.jar:/opt/kafka/bin/../libs/jersey-server-2.27.jar:/opt/kafka/bin/../libs/jetty-client-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-http-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-io-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-security-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-server-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-util-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka-clients-2.0.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.11-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.0.0.jar:/opt/kafka/bin/../libs/kafka-tools-2.0.0.jar:/opt/kafka/bin/../libs/kafka_2.11-2.0.0-sources.jar:/opt/kafka/bin/../libs/kafka_2.11-2.0.0.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.4.1.jar:/opt/kafka/bin/../libs/maven-artifact-3.5.3.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/opt/kafka/bin/../libs/plexus-utils-3.1.0.jar:/opt/kafka/bin/../libs/reflections-0.9.11.jar:/opt/kafka/bin/../libs/rocksdbjni-5.7.3.jar:/opt/kafka/bin/../libs/scala-library-2.11.12.jar:/opt/kafka/bin/../libs/scala-logging_2.11-3.9.0.jar:/opt/kafka/bin/../libs/scala-reflect-2.11.12.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.25.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.25.jar:/opt/kafka/bin/../libs/snappy-java-1.1.7.1.jar:/opt/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/opt/kafka/bin/../libs/zkclient-0.10.jar:/opt/kafka/bin/../libs/zookeeper-3.4.13.jar (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,208] INFO Client environment:java.library.path=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,211] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,211] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,211] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,212] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,217] INFO Client environment:os.version=4.15.0-43-generic (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,217] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,218] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,219] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 04:02:24,232] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@223d2c72 (org.apache.zookeeper.ZooKeeper)
[36mpostgres_1   |[0m LOG:  database system was shut down at 2018-12-30 04:02:21 UTC
[36mpostgres_1   |[0m LOG:  MultiXact member wraparound protections are now enabled
[36mpostgres_1   |[0m LOG:  database system is ready to accept connections
[36mpostgres_1   |[0m LOG:  autovacuum launcher started
[32mkafka_1      |[0m [2018-12-30 04:02:24,322] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 04:02:24,346] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,378 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60460
[32mkafka_1      |[0m [2018-12-30 04:02:24,390] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,413 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.22.0.4:60460
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,421 [myid:] - INFO  [SyncThread:0:FileTxnLog@199] - Creating new log file: log.1
[32mkafka_1      |[0m [2018-12-30 04:02:24,478] INFO Session establishment complete on server zookeeper/172.22.0.3:2181, sessionid = 0x167fd46c9780000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 04:02:24,483] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,495 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x167fd46c9780000 with negotiated timeout 6000 for client /172.22.0.4:60460
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,874 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,899 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
[33mzookeeper_1  |[0m 2018-12-30 04:02:24,918 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
[36mpostgres_1   |[0m  done
[36mpostgres_1   |[0m server started
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m CREATE ROLE
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-user-db.sh
[33mzookeeper_1  |[0m 2018-12-30 04:02:26,519 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
[32mkafka_1      |[0m [2018-12-30 04:02:26,553] INFO Cluster ID = LHOyrYJCS7aFYLgjYg6Yrg (kafka.server.KafkaServer)
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[32mkafka_1      |[0m [2018-12-30 04:02:26,614] WARN No meta.properties file under dir /kafka/kafka-logs-a1248bd306d5/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[32mkafka_1      |[0m [2018-12-30 04:02:27,130] INFO KafkaConfig values: 
[32mkafka_1      |[0m 	advertised.host.name = kafka
[32mkafka_1      |[0m 	advertised.listeners = null
[32mkafka_1      |[0m 	advertised.port = 9092
[32mkafka_1      |[0m 	alter.config.policy.class.name = null
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	authorizer.class.name = 
[32mkafka_1      |[0m 	auto.create.topics.enable = true
[32mkafka_1      |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1      |[0m 	background.threads = 10
[32mkafka_1      |[0m 	broker.id = -1
[32mkafka_1      |[0m 	broker.id.generation.enable = true
[32mkafka_1      |[0m 	broker.rack = null
[32mkafka_1      |[0m 	client.quota.callback.class = null
[32mkafka_1      |[0m 	compression.type = producer
[32mkafka_1      |[0m 	connections.max.idle.ms = 600000
[32mkafka_1      |[0m 	controlled.shutdown.enable = true
[32mkafka_1      |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1      |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1      |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	create.topic.policy.class.name = null
[32mkafka_1      |[0m 	default.replication.factor = 1
[32mkafka_1      |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1      |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1      |[0m 	delegation.token.master.key = null
[32mkafka_1      |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1      |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1      |[0m 	delete.topic.enable = true
[32mkafka_1      |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1      |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1      |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1      |[0m 	host.name = 
[32mkafka_1      |[0m 	inter.broker.listener.name = null
[32mkafka_1      |[0m 	inter.broker.protocol.version = 2.0-IV1
[32mkafka_1      |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1      |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1      |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[32mkafka_1      |[0m 	listeners = null
[32mkafka_1      |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1      |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1      |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1      |[0m 	log.cleaner.enable = true
[32mkafka_1      |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1      |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1      |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1      |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1      |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1      |[0m 	log.cleaner.threads = 1
[32mkafka_1      |[0m 	log.cleanup.policy = [delete]
[32mkafka_1      |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1      |[0m 	log.dirs = /kafka/kafka-logs-a1248bd306d5
[32mkafka_1      |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.interval.ms = null
[32mkafka_1      |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.index.interval.bytes = 4096
[32mkafka_1      |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1      |[0m 	log.message.downconversion.enable = true
[32mkafka_1      |[0m 	log.message.format.version = 2.0-IV1
[32mkafka_1      |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1      |[0m 	log.preallocate = false
[32mkafka_1      |[0m 	log.retention.bytes = -1
[32mkafka_1      |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1      |[0m 	log.retention.hours = 168
[32mkafka_1      |[0m 	log.retention.minutes = null
[32mkafka_1      |[0m 	log.retention.ms = null
[32mkafka_1      |[0m 	log.roll.hours = 168
[32mkafka_1      |[0m 	log.roll.jitter.hours = 0
[32mkafka_1      |[0m 	log.roll.jitter.ms = null
[32mkafka_1      |[0m 	log.roll.ms = null
[32mkafka_1      |[0m 	log.segment.bytes = 1073741824
[32mkafka_1      |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1      |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1      |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1      |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1      |[0m 	message.max.bytes = 1000012
[32mkafka_1      |[0m 	metric.reporters = []
[32mkafka_1      |[0m 	metrics.num.samples = 2
[32mkafka_1      |[0m 	metrics.recording.level = INFO
[32mkafka_1      |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1      |[0m 	min.insync.replicas = 1
[32mkafka_1      |[0m 	num.io.threads = 8
[32mkafka_1      |[0m 	num.network.threads = 3
[32mkafka_1      |[0m 	num.partitions = 1
[32mkafka_1      |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1      |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1      |[0m 	num.replica.fetchers = 1
[32mkafka_1      |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1      |[0m 	offsets.commit.required.acks = -1
[32mkafka_1      |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1      |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1      |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1      |[0m 	offsets.retention.minutes = 10080
[32mkafka_1      |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1      |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1      |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1      |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1      |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1      |[0m 	password.encoder.iterations = 4096
[32mkafka_1      |[0m 	password.encoder.key.length = 128
[32mkafka_1      |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1      |[0m 	password.encoder.old.secret = null
[32mkafka_1      |[0m 	password.encoder.secret = null
[32mkafka_1      |[0m 	port = 9092
[32mkafka_1      |[0m 	principal.builder.class = null
[32mkafka_1      |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	queued.max.request.bytes = -1
[32mkafka_1      |[0m 	queued.max.requests = 500
[32mkafka_1      |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.window.num = 11
[32mkafka_1      |[0m 	quota.window.size.seconds = 1
[32mkafka_1      |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1      |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1      |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1      |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1      |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1      |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1      |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1      |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1      |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	replication.quota.window.num = 11
[32mkafka_1      |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	request.timeout.ms = 30000
[32mkafka_1      |[0m 	reserved.broker.max.id = 1000
[32mkafka_1      |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1      |[0m 	sasl.jaas.config = null
[32mkafka_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1      |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1      |[0m 	sasl.kerberos.service.name = null
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.login.class = null
[32mkafka_1      |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1      |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1      |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1      |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1      |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1      |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1      |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1      |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1      |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1      |[0m 	ssl.cipher.suites = []
[32mkafka_1      |[0m 	ssl.client.auth = none
[32mkafka_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1      |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1      |[0m 	ssl.key.password = null
[32mkafka_1      |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1      |[0m 	ssl.keystore.location = null
[32mkafka_1      |[0m 	ssl.keystore.password = null
[32mkafka_1      |[0m 	ssl.keystore.type = JKS
[32mkafka_1      |[0m 	ssl.protocol = TLS
[32mkafka_1      |[0m 	ssl.provider = null
[32mkafka_1      |[0m 	ssl.secure.random.implementation = null
[32mkafka_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1      |[0m 	ssl.truststore.location = null
[32mkafka_1      |[0m 	ssl.truststore.password = null
[32mkafka_1      |[0m 	ssl.truststore.type = JKS
[32mkafka_1      |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1      |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1      |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1      |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1      |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1      |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1      |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1      |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1      |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1      |[0m 	unclean.leader.election.enable = false
[32mkafka_1      |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1      |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1      |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.set.acl = false
[32mkafka_1      |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1      |[0m  (kafka.server.KafkaConfig)
[32mkafka_1      |[0m [2018-12-30 04:02:27,258] INFO KafkaConfig values: 
[32mkafka_1      |[0m 	advertised.host.name = kafka
[32mkafka_1      |[0m 	advertised.listeners = null
[32mkafka_1      |[0m 	advertised.port = 9092
[32mkafka_1      |[0m 	alter.config.policy.class.name = null
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	authorizer.class.name = 
[32mkafka_1      |[0m 	auto.create.topics.enable = true
[32mkafka_1      |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1      |[0m 	background.threads = 10
[32mkafka_1      |[0m 	broker.id = -1
[32mkafka_1      |[0m 	broker.id.generation.enable = true
[32mkafka_1      |[0m 	broker.rack = null
[32mkafka_1      |[0m 	client.quota.callback.class = null
[32mkafka_1      |[0m 	compression.type = producer
[32mkafka_1      |[0m 	connections.max.idle.ms = 600000
[32mkafka_1      |[0m 	controlled.shutdown.enable = true
[32mkafka_1      |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1      |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1      |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	create.topic.policy.class.name = null
[32mkafka_1      |[0m 	default.replication.factor = 1
[32mkafka_1      |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1      |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1      |[0m 	delegation.token.master.key = null
[32mkafka_1      |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1      |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1      |[0m 	delete.topic.enable = true
[32mkafka_1      |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1      |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1      |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1      |[0m 	host.name = 
[32mkafka_1      |[0m 	inter.broker.listener.name = null
[32mkafka_1      |[0m 	inter.broker.protocol.version = 2.0-IV1
[32mkafka_1      |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1      |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1      |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[32mkafka_1      |[0m 	listeners = null
[32mkafka_1      |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1      |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1      |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1      |[0m 	log.cleaner.enable = true
[32mkafka_1      |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1      |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1      |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1      |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1      |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1      |[0m 	log.cleaner.threads = 1
[32mkafka_1      |[0m 	log.cleanup.policy = [delete]
[32mkafka_1      |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1      |[0m 	log.dirs = /kafka/kafka-logs-a1248bd306d5
[32mkafka_1      |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.interval.ms = null
[32mkafka_1      |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.index.interval.bytes = 4096
[32mkafka_1      |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1      |[0m 	log.message.downconversion.enable = true
[32mkafka_1      |[0m 	log.message.format.version = 2.0-IV1
[32mkafka_1      |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1      |[0m 	log.preallocate = false
[32mkafka_1      |[0m 	log.retention.bytes = -1
[32mkafka_1      |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1      |[0m 	log.retention.hours = 168
[32mkafka_1      |[0m 	log.retention.minutes = null
[32mkafka_1      |[0m 	log.retention.ms = null
[32mkafka_1      |[0m 	log.roll.hours = 168
[32mkafka_1      |[0m 	log.roll.jitter.hours = 0
[32mkafka_1      |[0m 	log.roll.jitter.ms = null
[32mkafka_1      |[0m 	log.roll.ms = null
[32mkafka_1      |[0m 	log.segment.bytes = 1073741824
[32mkafka_1      |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1      |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1      |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1      |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1      |[0m 	message.max.bytes = 1000012
[32mkafka_1      |[0m 	metric.reporters = []
[32mkafka_1      |[0m 	metrics.num.samples = 2
[32mkafka_1      |[0m 	metrics.recording.level = INFO
[32mkafka_1      |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1      |[0m 	min.insync.replicas = 1
[32mkafka_1      |[0m 	num.io.threads = 8
[32mkafka_1      |[0m 	num.network.threads = 3
[32mkafka_1      |[0m 	num.partitions = 1
[32mkafka_1      |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1      |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1      |[0m 	num.replica.fetchers = 1
[32mkafka_1      |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1      |[0m 	offsets.commit.required.acks = -1
[32mkafka_1      |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1      |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1      |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1      |[0m 	offsets.retention.minutes = 10080
[32mkafka_1      |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1      |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1      |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1      |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1      |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1      |[0m 	password.encoder.iterations = 4096
[32mkafka_1      |[0m 	password.encoder.key.length = 128
[32mkafka_1      |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1      |[0m 	password.encoder.old.secret = null
[32mkafka_1      |[0m 	password.encoder.secret = null
[32mkafka_1      |[0m 	port = 9092
[32mkafka_1      |[0m 	principal.builder.class = null
[32mkafka_1      |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	queued.max.request.bytes = -1
[32mkafka_1      |[0m 	queued.max.requests = 500
[32mkafka_1      |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.window.num = 11
[32mkafka_1      |[0m 	quota.window.size.seconds = 1
[32mkafka_1      |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1      |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1      |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1      |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1      |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1      |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1      |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1      |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1      |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	replication.quota.window.num = 11
[32mkafka_1      |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	request.timeout.ms = 30000
[32mkafka_1      |[0m 	reserved.broker.max.id = 1000
[32mkafka_1      |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1      |[0m 	sasl.jaas.config = null
[32mkafka_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1      |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1      |[0m 	sasl.kerberos.service.name = null
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.login.class = null
[32mkafka_1      |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1      |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1      |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1      |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1      |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1      |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1      |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1      |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1      |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1      |[0m 	ssl.cipher.suites = []
[32mkafka_1      |[0m 	ssl.client.auth = none
[32mkafka_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1      |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1      |[0m 	ssl.key.password = null
[32mkafka_1      |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1      |[0m 	ssl.keystore.location = null
[32mkafka_1      |[0m 	ssl.keystore.password = null
[32mkafka_1      |[0m 	ssl.keystore.type = JKS
[32mkafka_1      |[0m 	ssl.protocol = TLS
[32mkafka_1      |[0m 	ssl.provider = null
[32mkafka_1      |[0m 	ssl.secure.random.implementation = null
[32mkafka_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1      |[0m 	ssl.truststore.location = null
[32mkafka_1      |[0m 	ssl.truststore.password = null
[32mkafka_1      |[0m 	ssl.truststore.type = JKS
[32mkafka_1      |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1      |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1      |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1      |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1      |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1      |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1      |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1      |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1      |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1      |[0m 	unclean.leader.election.enable = false
[32mkafka_1      |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1      |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1      |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.set.acl = false
[32mkafka_1      |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1      |[0m  (kafka.server.KafkaConfig)
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[32mkafka_1      |[0m [2018-12-30 04:02:27,467] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:27,471] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:27,496] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:27,615] INFO Log directory /kafka/kafka-logs-a1248bd306d5 not found, creating it. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:27,676] INFO Loading logs. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:27,719] INFO Logs loading complete in 40 ms. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:27,789] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:27,798] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m LOG:  received fast shutdown request
[36mpostgres_1   |[0m LOG:  aborting any active transactions
[36mpostgres_1   |[0m LOG:  autovacuum launcher shutting down
[36mpostgres_1   |[0m LOG:  shutting down
[36mpostgres_1   |[0m waiting for server to shut down....LOG:  database system is shut down
[35mshipping_1   |[0m 
[35mshipping_1   |[0m   .   ____          _            __ _ _
[35mshipping_1   |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[35mshipping_1   |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[35mshipping_1   |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[35mshipping_1   |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[35mshipping_1   |[0m  =========|_|==============|___/=/_/_/_/
[35mshipping_1   |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[35mshipping_1   |[0m 
[36mpostgres_1   |[0m  done
[36mpostgres_1   |[0m server stopped
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m PostgreSQL init process complete; ready for start up.
[36mpostgres_1   |[0m 
[31morder_1      |[0m 
[36mpostgres_1   |[0m LOG:  database system was shut down at 2018-12-30 04:02:28 UTC
[36mpostgres_1   |[0m LOG:  MultiXact member wraparound protections are now enabled
[36mpostgres_1   |[0m LOG:  database system is ready to accept connections
[36mpostgres_1   |[0m LOG:  autovacuum launcher started
[31morder_1      |[0m   .   ____          _            __ _ _
[31morder_1      |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[31morder_1      |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[31morder_1      |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[31morder_1      |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[31morder_1      |[0m  =========|_|==============|___/=/_/_/_/
[31morder_1      |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[31morder_1      |[0m 
[34minvoicing_1  |[0m 
[34minvoicing_1  |[0m   .   ____          _            __ _ _
[34minvoicing_1  |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[34minvoicing_1  |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[34minvoicing_1  |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[34minvoicing_1  |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[34minvoicing_1  |[0m  =========|_|==============|___/=/_/_/_/
[34minvoicing_1  |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[34minvoicing_1  |[0m 
[32mkafka_1      |[0m [2018-12-30 04:02:30,595] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[35mshipping_1   |[0m 2018-12-30 04:02:30.546  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : Starting ShippingApp v0.0.1-SNAPSHOT on e2ae3a800577 with PID 6 (/microservice-kafka-shipping-0.0.1-SNAPSHOT.jar started by root in /)
[35mshipping_1   |[0m 2018-12-30 04:02:30.614  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : No active profile set, falling back to default profiles: default
[34minvoicing_1  |[0m 2018-12-30 04:02:30.652  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : Starting InvoiceApp v0.0.1-SNAPSHOT on 31cb49e77a91 with PID 6 (/microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar started by root in /)
[34minvoicing_1  |[0m 2018-12-30 04:02:30.743  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : No active profile set, falling back to default profiles: default
[31morder_1      |[0m 2018-12-30 04:02:30.760  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : Starting OrderApp v0.0.1-SNAPSHOT on afb08425f23c with PID 6 (/microservice-kafka-order-0.0.1-SNAPSHOT.jar started by root in /)
[31morder_1      |[0m 2018-12-30 04:02:30.846  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : No active profile set, falling back to default profiles: default
[32mkafka_1      |[0m [2018-12-30 04:02:30,963] INFO [SocketServer brokerId=1001] Started 1 acceptor threads (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-30 04:02:31,170] INFO [ExpirationReaper-1001-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:31,173] INFO [ExpirationReaper-1001-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:31,202] INFO [ExpirationReaper-1001-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:31,333] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1      |[0m creating topics: order:5:1
[32mkafka_1      |[0m [2018-12-30 04:02:31,865] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 04:02:31,907] INFO Result of znode creation at /brokers/ids/1001 is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 04:02:31,927] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 04:02:31,947] WARN No meta.properties file under dir /kafka/kafka-logs-a1248bd306d5/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[32mkafka_1      |[0m [2018-12-30 04:02:32,862] INFO [ExpirationReaper-1001-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:32,904] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 04:02:32,926] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 04:02:32,973] INFO [ExpirationReaper-1001-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-30 04:02:32,988] INFO [ExpirationReaper-1001-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[33mzookeeper_1  |[0m 2018-12-30 04:02:33,003 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:setData cxid:0x22 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
[32mkafka_1      |[0m [2018-12-30 04:02:33,222] INFO [GroupCoordinator 1001]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:02:33,278] INFO [GroupCoordinator 1001]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:02:33,309] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 50 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:02:33,420] INFO [ProducerId Manager 1001]: Acquired new producerId block (brokerId:1001,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[32mkafka_1      |[0m [2018-12-30 04:02:33,841] INFO [TransactionCoordinator id=1001] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:02:33,878] INFO [TransactionCoordinator id=1001] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:02:33,878] INFO [Transaction Marker Channel Manager 1001]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[32mkafka_1      |[0m [2018-12-30 04:02:34,670] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1      |[0m [2018-12-30 04:02:35,082] INFO [SocketServer brokerId=1001] Started processors for 1 acceptors (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-30 04:02:35,112] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[33mzookeeper_1  |[0m 2018-12-30 04:02:35,140 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:delete cxid:0x38 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions
[32mkafka_1      |[0m [2018-12-30 04:02:35,155] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka_1      |[0m [2018-12-30 04:02:35,163] INFO [KafkaServer id=1001] started (kafka.server.KafkaServer)
[34minvoicing_1  |[0m 2018-12-30 04:02:35.204  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@270421f5: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[33mzookeeper_1  |[0m 2018-12-30 04:02:35,242 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:delete cxid:0x3a zxid:0x1f txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[35mshipping_1   |[0m 2018-12-30 04:02:35.348  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@1c2c22f3: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[31morder_1      |[0m 2018-12-30 04:02:35.411  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@769c9116: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[33mzookeeper_1  |[0m 2018-12-30 04:02:43,296 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60464
[33mzookeeper_1  |[0m 2018-12-30 04:02:43,328 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.22.0.4:60464
[33mzookeeper_1  |[0m 2018-12-30 04:02:43,334 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x167fd46c9780001 with negotiated timeout 30000 for client /172.22.0.4:60464
[33mzookeeper_1  |[0m 2018-12-30 04:02:45,715 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780001 type:setData cxid:0x4 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/order Error:KeeperErrorCode = NoNode for /config/topics/order
[32mkafka_1      |[0m Created topic "order".
[33mzookeeper_1  |[0m 2018-12-30 04:02:46,188 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x167fd46c9780001
[33mzookeeper_1  |[0m 2018-12-30 04:02:46,196 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /172.22.0.4:60464 which had sessionid 0x167fd46c9780001
[32mkafka_1      |[0m [2018-12-30 04:02:47,018] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions order-2,order-1,order-4,order-0,order-3 (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,324] INFO [Log partition=order-4, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,361] INFO [Log partition=order-4, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 165 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,372] INFO Created log for partition order-4 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,378] INFO [Partition order-4 broker=1001] No checkpointed highwatermark is found for partition order-4 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,391] INFO Replica loaded for partition order-4 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:02:47,410] INFO [Partition order-4 broker=1001] order-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,508] INFO [Log partition=order-1, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,513] INFO [Log partition=order-1, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,518] INFO Created log for partition order-1 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,532] INFO [Partition order-1 broker=1001] No checkpointed highwatermark is found for partition order-1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,533] INFO Replica loaded for partition order-1 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:02:47,534] INFO [Partition order-1 broker=1001] order-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,561] INFO [Log partition=order-2, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,569] INFO [Log partition=order-2, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,584] INFO Created log for partition order-2 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,595] INFO [Partition order-2 broker=1001] No checkpointed highwatermark is found for partition order-2 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,595] INFO Replica loaded for partition order-2 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:02:47,596] INFO [Partition order-2 broker=1001] order-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,629] INFO [Log partition=order-3, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,642] INFO [Log partition=order-3, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,658] INFO Created log for partition order-3 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,659] INFO [Partition order-3 broker=1001] No checkpointed highwatermark is found for partition order-3 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,664] INFO Replica loaded for partition order-3 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:02:47,665] INFO [Partition order-3 broker=1001] order-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,698] INFO [Log partition=order-0, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,700] INFO [Log partition=order-0, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:02:47,707] INFO Created log for partition order-0 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:02:47,719] INFO [Partition order-0 broker=1001] No checkpointed highwatermark is found for partition order-0 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,721] INFO Replica loaded for partition order-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:02:47,726] INFO [Partition order-0 broker=1001] order-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:02:47,796] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[34minvoicing_1  |[0m 2018-12-30 04:02:54.927  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$86685842] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[35mshipping_1   |[0m 2018-12-30 04:02:55.083  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$259c8e9e] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[34minvoicing_1  |[0m 2018-12-30 04:02:55.783  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$b2b646bf] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[35mshipping_1   |[0m 2018-12-30 04:02:55.864  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$51ea7d1b] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[31morder_1      |[0m 2018-12-30 04:02:55.898  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$65f5d750] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[31morder_1      |[0m 2018-12-30 04:02:56.764  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$9243c5cd] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[31morder_1      |[0m 2018-12-30 04:02:57.214  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.hateoas.config.HateoasConfiguration' of type [org.springframework.hateoas.config.HateoasConfiguration$$EnhancerBySpringCGLIB$$11c412ff] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[35mshipping_1   |[0m 2018-12-30 04:03:01.713  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[34minvoicing_1  |[0m 2018-12-30 04:03:01.766  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[35mshipping_1   |[0m 2018-12-30 04:03:02.039  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[35mshipping_1   |[0m 2018-12-30 04:03:02.051  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[35mshipping_1   |[0m 2018-12-30 04:03:02.156  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[34minvoicing_1  |[0m 2018-12-30 04:03:02.195  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[34minvoicing_1  |[0m 2018-12-30 04:03:02.201  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[34minvoicing_1  |[0m 2018-12-30 04:03:02.319  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[31morder_1      |[0m 2018-12-30 04:03:02.524  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[31morder_1      |[0m 2018-12-30 04:03:02.929  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[31morder_1      |[0m 2018-12-30 04:03:02.939  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[35mshipping_1   |[0m 2018-12-30 04:03:02.968  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[35mshipping_1   |[0m 2018-12-30 04:03:02.976  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 27660 ms
[31morder_1      |[0m 2018-12-30 04:03:03.061  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[34minvoicing_1  |[0m 2018-12-30 04:03:03.464  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[34minvoicing_1  |[0m 2018-12-30 04:03:03.477  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 28316 ms
[31morder_1      |[0m 2018-12-30 04:03:04.084  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[31morder_1      |[0m 2018-12-30 04:03:04.108  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 28746 ms
[35mshipping_1   |[0m 2018-12-30 04:03:09.778  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.779  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.785  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.788  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.789  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.790  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-30 04:03:09.796  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.296  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.323  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.325  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.325  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.330  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.333  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[34minvoicing_1  |[0m 2018-12-30 04:03:10.334  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[35mshipping_1   |[0m 2018-12-30 04:03:10.856  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[35mshipping_1   |[0m 2018-12-30 04:03:11.560  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[34minvoicing_1  |[0m 2018-12-30 04:03:11.619  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[31morder_1      |[0m 2018-12-30 04:03:11.910  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.921  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.929  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.932  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.934  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.937  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[31morder_1      |[0m 2018-12-30 04:03:11.945  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[35mshipping_1   |[0m 2018-12-30 04:03:12.140  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[35mshipping_1   |[0m 2018-12-30 04:03:12.239  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[35mshipping_1   |[0m 	name: default
[35mshipping_1   |[0m 	...]
[34minvoicing_1  |[0m 2018-12-30 04:03:12.595  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[35mshipping_1   |[0m 2018-12-30 04:03:12.971  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[35mshipping_1   |[0m 2018-12-30 04:03:12.984  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[31morder_1      |[0m 2018-12-30 04:03:13.002  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[34minvoicing_1  |[0m 2018-12-30 04:03:13.419  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[35mshipping_1   |[0m 2018-12-30 04:03:13.489  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[34minvoicing_1  |[0m 2018-12-30 04:03:13.507  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[34minvoicing_1  |[0m 	name: default
[34minvoicing_1  |[0m 	...]
[31morder_1      |[0m 2018-12-30 04:03:13.937  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[34minvoicing_1  |[0m 2018-12-30 04:03:14.320  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[34minvoicing_1  |[0m 2018-12-30 04:03:14.333  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[31morder_1      |[0m 2018-12-30 04:03:14.757  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[31morder_1      |[0m 2018-12-30 04:03:15.038  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[31morder_1      |[0m 	name: default
[31morder_1      |[0m 	...]
[34minvoicing_1  |[0m 2018-12-30 04:03:15.321  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[35mshipping_1   |[0m 2018-12-30 04:03:16.201  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[31morder_1      |[0m 2018-12-30 04:03:16.513  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[31morder_1      |[0m 2018-12-30 04:03:16.528  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[31morder_1      |[0m 2018-12-30 04:03:16.976  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[35mshipping_1   |[0m 2018-12-30 04:03:17.377  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[35mshipping_1   |[0m 
[35mshipping_1   |[0m java.lang.reflect.InvocationTargetException: null
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at com.ewolff.microservice.shipping.ShippingApp.main(ShippingApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[35mshipping_1   |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[35mshipping_1   |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[35mshipping_1   |[0m 	... 52 common frames omitted
[35mshipping_1   |[0m 
[35mshipping_1   |[0m 2018-12-30 04:03:17.406  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@484094a5
[34minvoicing_1  |[0m 2018-12-30 04:03:18.201  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[31morder_1      |[0m 2018-12-30 04:03:19.103  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[34minvoicing_1  |[0m 2018-12-30 04:03:19.747  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[34minvoicing_1  |[0m 
[34minvoicing_1  |[0m java.lang.reflect.InvocationTargetException: null
[34minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34minvoicing_1  |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34minvoicing_1  |[0m 	at com.ewolff.microservice.invoicing.InvoiceApp.main(InvoiceApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[34minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[34minvoicing_1  |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34minvoicing_1  |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34minvoicing_1  |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34minvoicing_1  |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34minvoicing_1  |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[34minvoicing_1  |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[34minvoicing_1  |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[34minvoicing_1  |[0m 	... 52 common frames omitted
[34minvoicing_1  |[0m 
[34minvoicing_1  |[0m 2018-12-30 04:03:19.777  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@2c07545f
[31morder_1      |[0m 2018-12-30 04:03:20.255  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[31morder_1      |[0m 
[31morder_1      |[0m java.lang.reflect.InvocationTargetException: null
[31morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[31morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[31morder_1      |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[31morder_1      |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31morder_1      |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31morder_1      |[0m 	at com.ewolff.microservice.order.OrderApp.main(OrderApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[31morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[31morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[31morder_1      |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[31morder_1      |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[31morder_1      |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31morder_1      |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31morder_1      |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31morder_1      |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31morder_1      |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[31morder_1      |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[31morder_1      |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[31morder_1      |[0m 	... 52 common frames omitted
[31morder_1      |[0m 
[31morder_1      |[0m 2018-12-30 04:03:20.266  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@112f364d
[35mshipping_1   |[0m 2018-12-30 04:03:24.902  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[35mshipping_1   |[0m 2018-12-30 04:03:24.903  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_bhctnoxmrnws4tpfowks2n5ip" of relation "shipment_shipment_line" does not exist, skipping
[35mshipping_1   |[0m 2018-12-30 04:03:24.959  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[31morder_1      |[0m 2018-12-30 04:03:26.908  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[31morder_1      |[0m 2018-12-30 04:03:26.917  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_1y2g3x9fjij1fjms7fa8tvkic" of relation "ordertable_order_line" does not exist, skipping
[31morder_1      |[0m 2018-12-30 04:03:26.977  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[34minvoicing_1  |[0m 2018-12-30 04:03:27.346  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[34minvoicing_1  |[0m 2018-12-30 04:03:27.347  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_nftn7w4ignckl0hkgot8ayrxm" of relation "invoice_invoice_line" does not exist, skipping
[34minvoicing_1  |[0m 2018-12-30 04:03:27.407  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[35mshipping_1   |[0m 2018-12-30 04:03:28.535  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[31morder_1      |[0m 2018-12-30 04:03:30.082  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[34minvoicing_1  |[0m 2018-12-30 04:03:30.594  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[35mshipping_1   |[0m 2018-12-30 04:03:31.100  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34minvoicing_1  |[0m 2018-12-30 04:03:33.896  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[35mshipping_1   |[0m 2018-12-30 04:03:36.964  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@1c2c22f3: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[35mshipping_1   |[0m 2018-12-30 04:03:37.459  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[35mshipping_1   |[0m 2018-12-30 04:03:37.831  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.shipping.web.ShippingController.Item(long)
[35mshipping_1   |[0m 2018-12-30 04:03:37.852  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.shipping.web.ShippingController.ItemList()
[35mshipping_1   |[0m 2018-12-30 04:03:37.906  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[35mshipping_1   |[0m 2018-12-30 04:03:37.910  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[35mshipping_1   |[0m 2018-12-30 04:03:38.344  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[35mshipping_1   |[0m 2018-12-30 04:03:38.351  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34minvoicing_1  |[0m 2018-12-30 04:03:38.728  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@270421f5: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[34minvoicing_1  |[0m 2018-12-30 04:03:39.182  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[34minvoicing_1  |[0m 2018-12-30 04:03:39.553  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.invoicing.web.InvoiceController.ItemList()
[34minvoicing_1  |[0m 2018-12-30 04:03:39.595  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.invoicing.web.InvoiceController.Item(long)
[34minvoicing_1  |[0m 2018-12-30 04:03:39.600  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[34minvoicing_1  |[0m 2018-12-30 04:03:39.623  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[34minvoicing_1  |[0m 2018-12-30 04:03:39.974  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34minvoicing_1  |[0m 2018-12-30 04:03:39.978  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[31morder_1      |[0m 2018-12-30 04:03:40.863  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[31morder_1      |[0m 2018-12-30 04:03:42.731  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@769c9116: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[31morder_1      |[0m 2018-12-30 04:03:43.223  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[31morder_1      |[0m 2018-12-30 04:03:43.545  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/line],methods=[POST]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.addLine(com.ewolff.microservice.order.logic.Order)
[31morder_1      |[0m 2018-12-30 04:03:43.557  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.orderList()
[31morder_1      |[0m 2018-12-30 04:03:43.558  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/],methods=[POST]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.post(com.ewolff.microservice.order.logic.Order)
[31morder_1      |[0m 2018-12-30 04:03:43.565  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[DELETE]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.post(long)
[31morder_1      |[0m 2018-12-30 04:03:43.566  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.get(long)
[31morder_1      |[0m 2018-12-30 04:03:43.567  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/form.html],methods=[GET]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.form()
[31morder_1      |[0m 2018-12-30 04:03:43.597  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[31morder_1      |[0m 2018-12-30 04:03:43.605  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[31morder_1      |[0m 2018-12-30 04:03:43.917  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[31morder_1      |[0m 2018-12-30 04:03:43.926  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[31morder_1      |[0m 2018-12-30 04:03:44.073  INFO 6 --- [           main] .m.m.a.ExceptionHandlerExceptionResolver : Detected @ExceptionHandler methods in repositoryRestExceptionHandler
[35mshipping_1   |[0m 2018-12-30 04:03:47.134  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[35mshipping_1   |[0m 2018-12-30 04:03:47.246  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.258  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.262  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.269  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.271  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.280  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.284  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.285  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.294  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.300  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.301  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.320  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.324  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.326  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.335  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.337  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.343  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.349  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-30 04:03:47.361  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[35mshipping_1   |[0m 2018-12-30 04:03:48.011  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[35mshipping_1   |[0m 2018-12-30 04:03:48.021  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[35mshipping_1   |[0m 2018-12-30 04:03:48.073  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[34minvoicing_1  |[0m 2018-12-30 04:03:48.250  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[35mshipping_1   |[0m 2018-12-30 04:03:48.305  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[34minvoicing_1  |[0m 2018-12-30 04:03:48.353  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.362  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.363  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.378  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.380  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.383  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.384  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.384  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.386  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.393  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.396  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.405  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.407  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.421  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.425  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.430  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.445  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.446  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34minvoicing_1  |[0m 2018-12-30 04:03:48.449  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[35mshipping_1   |[0m 2018-12-30 04:03:48.562  INFO 6 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
[35mshipping_1   |[0m 	auto.commit.interval.ms = 5000
[35mshipping_1   |[0m 	auto.offset.reset = earliest
[35mshipping_1   |[0m 	bootstrap.servers = [kafka:9092]
[35mshipping_1   |[0m 	check.crcs = true
[35mshipping_1   |[0m 	client.id = 
[35mshipping_1   |[0m 	connections.max.idle.ms = 540000
[35mshipping_1   |[0m 	enable.auto.commit = false
[35mshipping_1   |[0m 	exclude.internal.topics = true
[35mshipping_1   |[0m 	fetch.max.bytes = 52428800
[35mshipping_1   |[0m 	fetch.max.wait.ms = 500
[35mshipping_1   |[0m 	fetch.min.bytes = 1
[35mshipping_1   |[0m 	group.id = shipping
[35mshipping_1   |[0m 	heartbeat.interval.ms = 3000
[35mshipping_1   |[0m 	interceptor.classes = null
[35mshipping_1   |[0m 	internal.leave.group.on.close = true
[35mshipping_1   |[0m 	isolation.level = read_uncommitted
[35mshipping_1   |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[35mshipping_1   |[0m 	max.partition.fetch.bytes = 1048576
[35mshipping_1   |[0m 	max.poll.interval.ms = 300000
[35mshipping_1   |[0m 	max.poll.records = 500
[35mshipping_1   |[0m 	metadata.max.age.ms = 300000
[35mshipping_1   |[0m 	metric.reporters = []
[35mshipping_1   |[0m 	metrics.num.samples = 2
[35mshipping_1   |[0m 	metrics.recording.level = INFO
[35mshipping_1   |[0m 	metrics.sample.window.ms = 30000
[35mshipping_1   |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[35mshipping_1   |[0m 	receive.buffer.bytes = 65536
[35mshipping_1   |[0m 	reconnect.backoff.max.ms = 1000
[35mshipping_1   |[0m 	reconnect.backoff.ms = 50
[35mshipping_1   |[0m 	request.timeout.ms = 305000
[35mshipping_1   |[0m 	retry.backoff.ms = 100
[35mshipping_1   |[0m 	sasl.jaas.config = null
[35mshipping_1   |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mshipping_1   |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mshipping_1   |[0m 	sasl.kerberos.service.name = null
[35mshipping_1   |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mshipping_1   |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mshipping_1   |[0m 	sasl.mechanism = GSSAPI
[35mshipping_1   |[0m 	security.protocol = PLAINTEXT
[35mshipping_1   |[0m 	send.buffer.bytes = 131072
[35mshipping_1   |[0m 	session.timeout.ms = 10000
[35mshipping_1   |[0m 	ssl.cipher.suites = null
[35mshipping_1   |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[35mshipping_1   |[0m 	ssl.endpoint.identification.algorithm = null
[35mshipping_1   |[0m 	ssl.key.password = null
[35mshipping_1   |[0m 	ssl.keymanager.algorithm = SunX509
[35mshipping_1   |[0m 	ssl.keystore.location = null
[35mshipping_1   |[0m 	ssl.keystore.password = null
[35mshipping_1   |[0m 	ssl.keystore.type = JKS
[35mshipping_1   |[0m 	ssl.protocol = TLS
[35mshipping_1   |[0m 	ssl.provider = null
[35mshipping_1   |[0m 	ssl.secure.random.implementation = null
[35mshipping_1   |[0m 	ssl.trustmanager.algorithm = PKIX
[35mshipping_1   |[0m 	ssl.truststore.location = null
[35mshipping_1   |[0m 	ssl.truststore.password = null
[35mshipping_1   |[0m 	ssl.truststore.type = JKS
[35mshipping_1   |[0m 	value.deserializer = class com.ewolff.microservice.shipping.events.ShipmentDeserializer
[35mshipping_1   |[0m 
[34minvoicing_1  |[0m 2018-12-30 04:03:49.007  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[34minvoicing_1  |[0m 2018-12-30 04:03:49.036  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[34minvoicing_1  |[0m 2018-12-30 04:03:49.073  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[34minvoicing_1  |[0m 2018-12-30 04:03:49.299  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[35mshipping_1   |[0m 2018-12-30 04:03:49.349  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[35mshipping_1   |[0m 2018-12-30 04:03:49.353  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[35mshipping_1   |[0m 2018-12-30 04:03:49.395  INFO 6 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
[34minvoicing_1  |[0m 2018-12-30 04:03:49.555  INFO 6 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
[34minvoicing_1  |[0m 	auto.commit.interval.ms = 5000
[34minvoicing_1  |[0m 	auto.offset.reset = earliest
[34minvoicing_1  |[0m 	bootstrap.servers = [kafka:9092]
[34minvoicing_1  |[0m 	check.crcs = true
[34minvoicing_1  |[0m 	client.id = 
[34minvoicing_1  |[0m 	connections.max.idle.ms = 540000
[34minvoicing_1  |[0m 	enable.auto.commit = false
[34minvoicing_1  |[0m 	exclude.internal.topics = true
[34minvoicing_1  |[0m 	fetch.max.bytes = 52428800
[34minvoicing_1  |[0m 	fetch.max.wait.ms = 500
[34minvoicing_1  |[0m 	fetch.min.bytes = 1
[34minvoicing_1  |[0m 	group.id = invoicing
[34minvoicing_1  |[0m 	heartbeat.interval.ms = 3000
[34minvoicing_1  |[0m 	interceptor.classes = null
[34minvoicing_1  |[0m 	internal.leave.group.on.close = true
[34minvoicing_1  |[0m 	isolation.level = read_uncommitted
[34minvoicing_1  |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[34minvoicing_1  |[0m 	max.partition.fetch.bytes = 1048576
[34minvoicing_1  |[0m 	max.poll.interval.ms = 300000
[34minvoicing_1  |[0m 	max.poll.records = 500
[34minvoicing_1  |[0m 	metadata.max.age.ms = 300000
[34minvoicing_1  |[0m 	metric.reporters = []
[34minvoicing_1  |[0m 	metrics.num.samples = 2
[34minvoicing_1  |[0m 	metrics.recording.level = INFO
[34minvoicing_1  |[0m 	metrics.sample.window.ms = 30000
[34minvoicing_1  |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[34minvoicing_1  |[0m 	receive.buffer.bytes = 65536
[34minvoicing_1  |[0m 	reconnect.backoff.max.ms = 1000
[34minvoicing_1  |[0m 	reconnect.backoff.ms = 50
[34minvoicing_1  |[0m 	request.timeout.ms = 305000
[34minvoicing_1  |[0m 	retry.backoff.ms = 100
[34minvoicing_1  |[0m 	sasl.jaas.config = null
[34minvoicing_1  |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[34minvoicing_1  |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[34minvoicing_1  |[0m 	sasl.kerberos.service.name = null
[34minvoicing_1  |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[34minvoicing_1  |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[34minvoicing_1  |[0m 	sasl.mechanism = GSSAPI
[34minvoicing_1  |[0m 	security.protocol = PLAINTEXT
[34minvoicing_1  |[0m 	send.buffer.bytes = 131072
[34minvoicing_1  |[0m 	session.timeout.ms = 10000
[34minvoicing_1  |[0m 	ssl.cipher.suites = null
[34minvoicing_1  |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[34minvoicing_1  |[0m 	ssl.endpoint.identification.algorithm = null
[34minvoicing_1  |[0m 	ssl.key.password = null
[34minvoicing_1  |[0m 	ssl.keymanager.algorithm = SunX509
[34minvoicing_1  |[0m 	ssl.keystore.location = null
[34minvoicing_1  |[0m 	ssl.keystore.password = null
[34minvoicing_1  |[0m 	ssl.keystore.type = JKS
[34minvoicing_1  |[0m 	ssl.protocol = TLS
[34minvoicing_1  |[0m 	ssl.provider = null
[34minvoicing_1  |[0m 	ssl.secure.random.implementation = null
[34minvoicing_1  |[0m 	ssl.trustmanager.algorithm = PKIX
[34minvoicing_1  |[0m 	ssl.truststore.location = null
[34minvoicing_1  |[0m 	ssl.truststore.password = null
[34minvoicing_1  |[0m 	ssl.truststore.type = JKS
[34minvoicing_1  |[0m 	value.deserializer = class com.ewolff.microservice.invoicing.events.InvoiceDeserializer
[34minvoicing_1  |[0m 
[35mshipping_1   |[0m 2018-12-30 04:03:49.818  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[35mshipping_1   |[0m 2018-12-30 04:03:49.848  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : Started ShippingApp in 86.837 seconds (JVM running for 95.356)
[34minvoicing_1  |[0m 2018-12-30 04:03:50.141  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[34minvoicing_1  |[0m 2018-12-30 04:03:50.142  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[34minvoicing_1  |[0m 2018-12-30 04:03:50.180  INFO 6 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
[33mzookeeper_1  |[0m 2018-12-30 04:03:50,739 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780000 type:setData cxid:0x55 zxid:0x30 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
[32mkafka_1      |[0m [2018-12-30 04:03:50,754] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(1001), __consumer_offsets-30 -> ArrayBuffer(1001), __consumer_offsets-8 -> ArrayBuffer(1001), __consumer_offsets-21 -> ArrayBuffer(1001), __consumer_offsets-4 -> ArrayBuffer(1001), __consumer_offsets-27 -> ArrayBuffer(1001), __consumer_offsets-7 -> ArrayBuffer(1001), __consumer_offsets-9 -> ArrayBuffer(1001), __consumer_offsets-46 -> ArrayBuffer(1001), __consumer_offsets-25 -> ArrayBuffer(1001), __consumer_offsets-35 -> ArrayBuffer(1001), __consumer_offsets-41 -> ArrayBuffer(1001), __consumer_offsets-33 -> ArrayBuffer(1001), __consumer_offsets-23 -> ArrayBuffer(1001), __consumer_offsets-49 -> ArrayBuffer(1001), __consumer_offsets-47 -> ArrayBuffer(1001), __consumer_offsets-16 -> ArrayBuffer(1001), __consumer_offsets-28 -> ArrayBuffer(1001), __consumer_offsets-31 -> ArrayBuffer(1001), __consumer_offsets-36 -> ArrayBuffer(1001), __consumer_offsets-42 -> ArrayBuffer(1001), __consumer_offsets-3 -> ArrayBuffer(1001), __consumer_offsets-18 -> ArrayBuffer(1001), __consumer_offsets-37 -> ArrayBuffer(1001), __consumer_offsets-15 -> ArrayBuffer(1001), __consumer_offsets-24 -> ArrayBuffer(1001), __consumer_offsets-38 -> ArrayBuffer(1001), __consumer_offsets-17 -> ArrayBuffer(1001), __consumer_offsets-48 -> ArrayBuffer(1001), __consumer_offsets-19 -> ArrayBuffer(1001), __consumer_offsets-11 -> ArrayBuffer(1001), __consumer_offsets-13 -> ArrayBuffer(1001), __consumer_offsets-2 -> ArrayBuffer(1001), __consumer_offsets-43 -> ArrayBuffer(1001), __consumer_offsets-6 -> ArrayBuffer(1001), __consumer_offsets-14 -> ArrayBuffer(1001), __consumer_offsets-20 -> ArrayBuffer(1001), __consumer_offsets-0 -> ArrayBuffer(1001), __consumer_offsets-44 -> ArrayBuffer(1001), __consumer_offsets-39 -> ArrayBuffer(1001), __consumer_offsets-12 -> ArrayBuffer(1001), __consumer_offsets-45 -> ArrayBuffer(1001), __consumer_offsets-1 -> ArrayBuffer(1001), __consumer_offsets-5 -> ArrayBuffer(1001), __consumer_offsets-26 -> ArrayBuffer(1001), __consumer_offsets-29 -> ArrayBuffer(1001), __consumer_offsets-34 -> ArrayBuffer(1001), __consumer_offsets-10 -> ArrayBuffer(1001), __consumer_offsets-32 -> ArrayBuffer(1001), __consumer_offsets-40 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[34minvoicing_1  |[0m 2018-12-30 04:03:50.809  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[32mkafka_1      |[0m [2018-12-30 04:03:50,824] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[34minvoicing_1  |[0m 2018-12-30 04:03:50.857  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : Started InvoiceApp in 87.525 seconds (JVM running for 95.709)
[31morder_1      |[0m 2018-12-30 04:03:50.957  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerAdapter   : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@769c9116: startup date [Sun Dec 30 04:02:35 GMT 2018]; root of context hierarchy
[31morder_1      |[0m 2018-12-30 04:03:51.083  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositoryController.optionsForRepositories()
[31morder_1      |[0m 2018-12-30 04:03:51.092  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryController.headForRepositories()
[31morder_1      |[0m 2018-12-30 04:03:51.092  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RepositoryLinksResource> org.springframework.data.rest.webmvc.RepositoryController.listRepositories()
[31morder_1      |[0m 2018-12-30 04:03:51.145  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.151  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable) throws org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.155  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.181  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],produces=[application/x-spring-data-compact+json || text/uri-list]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResourceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.195  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[POST],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.postCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.196  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.203  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.230  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.Resource<?>> org.springframework.data.rest.webmvc.RepositoryEntityController.getItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.http.HttpHeaders) throws org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.232  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PUT],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.putItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.data.rest.webmvc.support.ETag,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.237  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PATCH],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.patchItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.data.rest.webmvc.support.ETag,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException,org.springframework.data.rest.webmvc.ResourceNotFoundException
[31morder_1      |[0m 2018-12-30 04:03:51.238  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.deleteItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.support.ETag) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[31morder_1      |[0m 2018-12-30 04:03:51.270  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.276  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.283  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.296  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],produces=[application/x-spring-data-compact+json || text/uri-list]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReferenceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.298  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[PATCH || PUT || POST],consumes=[application/json || application/x-spring-data-compact+json || text/uri-list],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.createPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.http.HttpMethod,org.springframework.hateoas.Resources<java.lang.Object>,java.io.Serializable,java.lang.String) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.310  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReferenceId(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String) throws java.lang.Exception
[31morder_1      |[0m 2018-12-30 04:03:51.339  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.344  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.346  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.data.rest.webmvc.RepositorySearchesResource org.springframework.data.rest.webmvc.RepositorySearchController.listSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.348  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.executeSearch(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.util.MultiValueMap<java.lang.String, java.lang.Object>,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.http.HttpHeaders)
[31morder_1      |[0m 2018-12-30 04:03:51.350  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],produces=[application/x-spring-data-compact+json]}" onto public org.springframework.hateoas.ResourceSupport org.springframework.data.rest.webmvc.RepositorySearchController.executeSearchCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.http.HttpHeaders,org.springframework.util.MultiValueMap<java.lang.String, java.lang.Object>,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler)
[31morder_1      |[0m 2018-12-30 04:03:51.365  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
[31morder_1      |[0m 2018-12-30 04:03:51.368  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
[31morder_1      |[0m 2018-12-30 04:03:51.410  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile],methods=[OPTIONS]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.ProfileController.profileOptions()
[31morder_1      |[0m 2018-12-30 04:03:51.414  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile],methods=[GET]}" onto org.springframework.http.HttpEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.ProfileController.listAllFormsOfMetadata()
[31morder_1      |[0m 2018-12-30 04:03:51.429  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[GET],produces=[application/alps+json || */*]}" onto org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RootResourceInformation> org.springframework.data.rest.webmvc.alps.AlpsController.descriptor(org.springframework.data.rest.webmvc.RootResourceInformation)
[31morder_1      |[0m 2018-12-30 04:03:51.433  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[OPTIONS],produces=[application/alps+json]}" onto org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.alps.AlpsController.alpsOptions()
[31morder_1      |[0m 2018-12-30 04:03:51.437  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[GET],produces=[application/schema+json]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.json.JsonSchema> org.springframework.data.rest.webmvc.RepositorySchemaController.schema(org.springframework.data.rest.webmvc.RootResourceInformation)
[32mkafka_1      |[0m [2018-12-30 04:03:52,857] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-30 04:03:52,954] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:52,988] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 34 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:52,990] INFO Created log for partition __consumer_offsets-0 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,022] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,040] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,060] INFO [Partition __consumer_offsets-0 broker=1001] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,067] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,087] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,093] INFO Created log for partition __consumer_offsets-29 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,160] INFO [Partition __consumer_offsets-29 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,162] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,163] INFO [Partition __consumer_offsets-29 broker=1001] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:53.191  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[32mkafka_1      |[0m [2018-12-30 04:03:53,199] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,238] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 69 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,259] INFO Created log for partition __consumer_offsets-48 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,266] INFO [Partition __consumer_offsets-48 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,293] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,293] INFO [Partition __consumer_offsets-48 broker=1001] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,299] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,309] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,314] INFO Created log for partition __consumer_offsets-10 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[31morder_1      |[0m 2018-12-30 04:03:53.335  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.340  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-30 04:03:53,353] INFO [Partition __consumer_offsets-10 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:53.356  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.363  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.367  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.368  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.373  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.374  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-30 04:03:53,374] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,376] INFO [Partition __consumer_offsets-10 broker=1001] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:53.380  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.381  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.386  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.393  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-30 04:03:53,398] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[31morder_1      |[0m 2018-12-30 04:03:53.399  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.401  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.402  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.409  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-30 04:03:53,412] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[31morder_1      |[0m 2018-12-30 04:03:53.413  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.415  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31morder_1      |[0m 2018-12-30 04:03:53.422  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[32mkafka_1      |[0m [2018-12-30 04:03:53,446] INFO Created log for partition __consumer_offsets-45 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,450] INFO [Partition __consumer_offsets-45 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,480] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,481] INFO [Partition __consumer_offsets-45 broker=1001] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,502] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,504] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,514] INFO Created log for partition __consumer_offsets-26 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,522] INFO [Partition __consumer_offsets-26 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,555] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,556] INFO [Partition __consumer_offsets-26 broker=1001] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,571] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,598] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,613] INFO Created log for partition __consumer_offsets-7 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,615] INFO [Partition __consumer_offsets-7 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,615] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,616] INFO [Partition __consumer_offsets-7 broker=1001] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,661] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,663] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,674] INFO Created log for partition __consumer_offsets-42 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,697] INFO [Partition __consumer_offsets-42 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,704] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,709] INFO [Partition __consumer_offsets-42 broker=1001] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,756] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,770] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,802] INFO Created log for partition __consumer_offsets-4 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,816] INFO [Partition __consumer_offsets-4 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,817] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,817] INFO [Partition __consumer_offsets-4 broker=1001] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,841] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,864] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,865] INFO Created log for partition __consumer_offsets-23 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,874] INFO [Partition __consumer_offsets-23 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,877] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,884] INFO [Partition __consumer_offsets-23 broker=1001] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,919] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,927] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:53,946] INFO Created log for partition __consumer_offsets-1 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:53,967] INFO [Partition __consumer_offsets-1 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:53,972] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:53,977] INFO [Partition __consumer_offsets-1 broker=1001] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,002] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,028] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,030] INFO Created log for partition __consumer_offsets-20 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,039] INFO [Partition __consumer_offsets-20 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,052] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,073] INFO [Partition __consumer_offsets-20 broker=1001] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,085] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,089] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,096] INFO Created log for partition __consumer_offsets-39 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,100] INFO [Partition __consumer_offsets-39 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,102] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,102] INFO [Partition __consumer_offsets-39 broker=1001] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,131] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,143] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,146] INFO Created log for partition __consumer_offsets-17 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,150] INFO [Partition __consumer_offsets-17 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,156] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,157] INFO [Partition __consumer_offsets-17 broker=1001] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,196] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,197] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,198] INFO Created log for partition __consumer_offsets-36 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[31morder_1      |[0m 2018-12-30 04:03:54.199  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[32mkafka_1      |[0m [2018-12-30 04:03:54,202] INFO [Partition __consumer_offsets-36 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:54.207  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[32mkafka_1      |[0m [2018-12-30 04:03:54,208] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,209] INFO [Partition __consumer_offsets-36 broker=1001] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,239] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,240] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,247] INFO Created log for partition __consumer_offsets-14 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,253] INFO [Partition __consumer_offsets-14 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,253] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,254] INFO [Partition __consumer_offsets-14 broker=1001] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:54.259  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[32mkafka_1      |[0m [2018-12-30 04:03:54,278] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,283] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,290] INFO Created log for partition __consumer_offsets-33 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,293] INFO [Partition __consumer_offsets-33 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,299] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,300] INFO [Partition __consumer_offsets-33 broker=1001] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:54.311  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[32mkafka_1      |[0m [2018-12-30 04:03:54,314] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,337] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,338] INFO Created log for partition __consumer_offsets-49 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,339] INFO [Partition __consumer_offsets-49 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,340] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,341] INFO [Partition __consumer_offsets-49 broker=1001] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,362] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,364] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,379] INFO Created log for partition __consumer_offsets-11 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,385] INFO [Partition __consumer_offsets-11 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,394] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,394] INFO [Partition __consumer_offsets-11 broker=1001] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,409] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,414] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 13 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,418] INFO Created log for partition __consumer_offsets-30 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,435] INFO [Partition __consumer_offsets-30 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,444] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,445] INFO [Partition __consumer_offsets-30 broker=1001] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,456] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,459] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,461] INFO Created log for partition __consumer_offsets-46 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,464] INFO [Partition __consumer_offsets-46 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,466] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,466] INFO [Partition __consumer_offsets-46 broker=1001] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,514] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,517] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,519] INFO Created log for partition __consumer_offsets-27 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,519] INFO [Partition __consumer_offsets-27 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,520] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,536] INFO [Partition __consumer_offsets-27 broker=1001] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,548] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,551] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 9 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,561] INFO Created log for partition __consumer_offsets-8 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,565] INFO [Partition __consumer_offsets-8 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,565] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,566] INFO [Partition __consumer_offsets-8 broker=1001] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[31morder_1      |[0m 2018-12-30 04:03:54.560  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[31morder_1      |[0m 2018-12-30 04:03:54.576  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : Started OrderApp in 91.228 seconds (JVM running for 99.425)
[32mkafka_1      |[0m [2018-12-30 04:03:54,578] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,613] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,615] INFO Created log for partition __consumer_offsets-24 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,618] INFO [Partition __consumer_offsets-24 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,621] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,622] INFO [Partition __consumer_offsets-24 broker=1001] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,656] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,659] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,660] INFO Created log for partition __consumer_offsets-43 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,664] INFO [Partition __consumer_offsets-43 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,669] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,669] INFO [Partition __consumer_offsets-43 broker=1001] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,689] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,706] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,714] INFO Created log for partition __consumer_offsets-5 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,715] INFO [Partition __consumer_offsets-5 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,721] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,725] INFO [Partition __consumer_offsets-5 broker=1001] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,753] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,755] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,756] INFO Created log for partition __consumer_offsets-21 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,764] INFO [Partition __consumer_offsets-21 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,781] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,781] INFO [Partition __consumer_offsets-21 broker=1001] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,787] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,794] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,808] INFO Created log for partition __consumer_offsets-2 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,810] INFO [Partition __consumer_offsets-2 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,817] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,818] INFO [Partition __consumer_offsets-2 broker=1001] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,830] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,833] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,866] INFO Created log for partition __consumer_offsets-40 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,867] INFO [Partition __consumer_offsets-40 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,869] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,869] INFO [Partition __consumer_offsets-40 broker=1001] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,878] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,888] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:54,890] INFO Created log for partition __consumer_offsets-37 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:54,894] INFO [Partition __consumer_offsets-37 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:54,894] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:54,895] INFO [Partition __consumer_offsets-37 broker=1001] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,070] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,071] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 155 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,072] INFO Created log for partition __consumer_offsets-18 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,073] INFO [Partition __consumer_offsets-18 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,084] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,085] INFO [Partition __consumer_offsets-18 broker=1001] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,105] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,106] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,108] INFO Created log for partition __consumer_offsets-34 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,115] INFO [Partition __consumer_offsets-34 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,121] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,121] INFO [Partition __consumer_offsets-34 broker=1001] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,131] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,138] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,140] INFO Created log for partition __consumer_offsets-15 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,187] INFO [Partition __consumer_offsets-15 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,188] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,188] INFO [Partition __consumer_offsets-15 broker=1001] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,195] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,201] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,210] INFO Created log for partition __consumer_offsets-12 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,211] INFO [Partition __consumer_offsets-12 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,211] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,211] INFO [Partition __consumer_offsets-12 broker=1001] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,220] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,229] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,230] INFO Created log for partition __consumer_offsets-31 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,233] INFO [Partition __consumer_offsets-31 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,236] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,236] INFO [Partition __consumer_offsets-31 broker=1001] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,255] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,258] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,260] INFO Created log for partition __consumer_offsets-9 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,262] INFO [Partition __consumer_offsets-9 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,265] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,266] INFO [Partition __consumer_offsets-9 broker=1001] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,298] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,305] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,310] INFO Created log for partition __consumer_offsets-47 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,313] INFO [Partition __consumer_offsets-47 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,317] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,319] INFO [Partition __consumer_offsets-47 broker=1001] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,335] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,342] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,349] INFO Created log for partition __consumer_offsets-19 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,351] INFO [Partition __consumer_offsets-19 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,361] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,361] INFO [Partition __consumer_offsets-19 broker=1001] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,395] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,401] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,403] INFO Created log for partition __consumer_offsets-28 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,407] INFO [Partition __consumer_offsets-28 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,410] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,411] INFO [Partition __consumer_offsets-28 broker=1001] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,431] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,432] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,433] INFO Created log for partition __consumer_offsets-38 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,434] INFO [Partition __consumer_offsets-38 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,438] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,440] INFO [Partition __consumer_offsets-38 broker=1001] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,452] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,457] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,459] INFO Created log for partition __consumer_offsets-35 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,463] INFO [Partition __consumer_offsets-35 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,463] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,466] INFO [Partition __consumer_offsets-35 broker=1001] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,503] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,512] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,519] INFO Created log for partition __consumer_offsets-44 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,522] INFO [Partition __consumer_offsets-44 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,527] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,542] INFO [Partition __consumer_offsets-44 broker=1001] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,561] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,562] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,570] INFO Created log for partition __consumer_offsets-6 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,571] INFO [Partition __consumer_offsets-6 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,572] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,573] INFO [Partition __consumer_offsets-6 broker=1001] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,607] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,630] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,633] INFO Created log for partition __consumer_offsets-25 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,645] INFO [Partition __consumer_offsets-25 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,648] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,648] INFO [Partition __consumer_offsets-25 broker=1001] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,666] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,670] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,674] INFO Created log for partition __consumer_offsets-16 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,676] INFO [Partition __consumer_offsets-16 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,677] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,679] INFO [Partition __consumer_offsets-16 broker=1001] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,727] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,733] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 12 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,740] INFO Created log for partition __consumer_offsets-22 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,741] INFO [Partition __consumer_offsets-22 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,742] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,742] INFO [Partition __consumer_offsets-22 broker=1001] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,748] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,761] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,777] INFO Created log for partition __consumer_offsets-41 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,782] INFO [Partition __consumer_offsets-41 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,783] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,783] INFO [Partition __consumer_offsets-41 broker=1001] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,807] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,826] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 40 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,827] INFO Created log for partition __consumer_offsets-32 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,845] INFO [Partition __consumer_offsets-32 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,849] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,849] INFO [Partition __consumer_offsets-32 broker=1001] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,860] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,861] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,866] INFO Created log for partition __consumer_offsets-3 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,870] INFO [Partition __consumer_offsets-3 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,871] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,872] INFO [Partition __consumer_offsets-3 broker=1001] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,880] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-a1248bd306d5] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,901] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-a1248bd306d5] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 22 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-30 04:03:55,904] INFO Created log for partition __consumer_offsets-13 in /kafka/kafka-logs-a1248bd306d5 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,908] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,924] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-30 04:03:55,924] INFO [Partition __consumer_offsets-13 broker=1001] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-30 04:03:55,930] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,943] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,960] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,964] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,966] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,966] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,966] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,966] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,967] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,967] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,967] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,967] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,967] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,968] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,968] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,968] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,970] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,976] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,976] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,977] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,978] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,979] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,979] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,980] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,980] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,980] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,980] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,980] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,981] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,981] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,982] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,987] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,987] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,988] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,983] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 4 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,990] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,990] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,990] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,991] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-37 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:55,992] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-40 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,007] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,008] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,011] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,011] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,007] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,012] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,012] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,012] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,013] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,013] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,013] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,013] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,013] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,014] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,026] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,027] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,027] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,027] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,027] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,028] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,028] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,029] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,028] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,036] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,037] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,040] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,041] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,061] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,061] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,063] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,063] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,064] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,064] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,068] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-8 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,068] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,068] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,069] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-17 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,069] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,069] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,071] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,072] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,074] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,075] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,075] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,080] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,081] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,081] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,081] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,082] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,093] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,094] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,094] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,094] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,095] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-27 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mshipping_1   |[0m 2018-12-30 04:03:56.103  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[35mshipping_1   |[0m 2018-12-30 04:03:56.107  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Revoking previously assigned partitions []
[35mshipping_1   |[0m 2018-12-30 04:03:56.110  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions revoked: []
[35mshipping_1   |[0m 2018-12-30 04:03:56.111  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] (Re-)joining group
[32mkafka_1      |[0m [2018-12-30 04:03:56,123] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34minvoicing_1  |[0m 2018-12-30 04:03:56.132  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[34minvoicing_1  |[0m 2018-12-30 04:03:56.136  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Revoking previously assigned partitions []
[32mkafka_1      |[0m [2018-12-30 04:03:56,138] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34minvoicing_1  |[0m 2018-12-30 04:03:56.141  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions revoked: []
[32mkafka_1      |[0m [2018-12-30 04:03:56,144] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,145] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,145] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,146] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:03:56,147] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-48 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34minvoicing_1  |[0m 2018-12-30 04:03:56.144  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] (Re-)joining group
[32mkafka_1      |[0m [2018-12-30 04:03:56,239] INFO [GroupCoordinator 1001]: Preparing to rebalance group invoicing with old generation 0 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,250] INFO [GroupCoordinator 1001]: Preparing to rebalance group shipping with old generation 0 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,284] INFO [GroupCoordinator 1001]: Stabilized group shipping generation 1 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,334] INFO [GroupCoordinator 1001]: Assignment received from leader for group shipping for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,344] INFO [GroupCoordinator 1001]: Stabilized group invoicing generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,361] INFO [GroupCoordinator 1001]: Assignment received from leader for group invoicing for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-30 04:03:56,431] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[32mkafka_1      |[0m [2018-12-30 04:03:56,431] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-8. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[34minvoicing_1  |[0m 2018-12-30 04:03:56.476  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Successfully joined group with generation 1
[34minvoicing_1  |[0m 2018-12-30 04:03:56.479  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Setting newly assigned partitions [order-4, order-2, order-3, order-0, order-1]
[35mshipping_1   |[0m 2018-12-30 04:03:56.490  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Successfully joined group with generation 1
[35mshipping_1   |[0m 2018-12-30 04:03:56.492  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Setting newly assigned partitions [order-4, order-2, order-3, order-0, order-1]
[35mshipping_1   |[0m 2018-12-30 04:03:56.586  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions assigned: [order-4, order-2, order-3, order-0, order-1]
[34minvoicing_1  |[0m 2018-12-30 04:03:56.593  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions assigned: [order-4, order-2, order-3, order-0, order-1]
[32mkafka_1      |[0m [2018-12-30 04:12:33,251] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mshipping_1   |[0m 2018-12-30 04:18:23.112  INFO 6 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
[35mshipping_1   |[0m 2018-12-30 04:18:23.112  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
[35mshipping_1   |[0m 2018-12-30 04:18:23.208  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 96 ms
[35mshipping_1   |[0m 2018-12-30 04:18:25.596  WARN 6 --- [nio-8080-exec-1] n.n.u.t.decorators.DecoratorProcessor    : The layout:decorator/data-layout-decorator processor has been deprecated and will be removed in the next major version of the layout dialect.  Please use layout:decorate/data-layout-decorate instead to future-proof your code.  See https://github.com/ultraq/thymeleaf-layout-dialect/issues/95 for more information.
[35mshipping_1   |[0m 2018-12-30 04:18:26.164  WARN 6 --- [nio-8080-exec-1] n.n.u.t.expressions.ExpressionProcessor  : Fragment expression "layout" is being wrapped as a Thymeleaf 3 fragment expression (~{...}) for backwards compatibility purposes.  This wrapping will be dropped in the next major version of the expression processor, so please rewrite as a Thymeleaf 3 fragment expression to future-proof your code.  See https://github.com/thymeleaf/thymeleaf/issues/451 for more information.
[31morder_1      |[0m 2018-12-30 04:18:32.040  INFO 6 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
[31morder_1      |[0m 2018-12-30 04:18:32.045  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
[31morder_1      |[0m 2018-12-30 04:18:32.251  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 205 ms
[31morder_1      |[0m 2018-12-30 04:18:35.295  WARN 6 --- [nio-8080-exec-1] n.n.u.t.decorators.DecoratorProcessor    : The layout:decorator/data-layout-decorator processor has been deprecated and will be removed in the next major version of the layout dialect.  Please use layout:decorate/data-layout-decorate instead to future-proof your code.  See https://github.com/ultraq/thymeleaf-layout-dialect/issues/95 for more information.
[31morder_1      |[0m 2018-12-30 04:18:35.958  WARN 6 --- [nio-8080-exec-1] n.n.u.t.expressions.ExpressionProcessor  : Fragment expression "layout" is being wrapped as a Thymeleaf 3 fragment expression (~{...}) for backwards compatibility purposes.  This wrapping will be dropped in the next major version of the expression processor, so please rewrite as a Thymeleaf 3 fragment expression to future-proof your code.  See https://github.com/thymeleaf/thymeleaf/issues/451 for more information.
[31morder_1      |[0m 2018-12-30 04:19:16.292  INFO 6 --- [nio-8080-exec-5] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
[31morder_1      |[0m 	acks = 1
[31morder_1      |[0m 	batch.size = 16384
[31morder_1      |[0m 	bootstrap.servers = [kafka:9092]
[31morder_1      |[0m 	buffer.memory = 33554432
[31morder_1      |[0m 	client.id = 
[31morder_1      |[0m 	compression.type = none
[31morder_1      |[0m 	connections.max.idle.ms = 540000
[31morder_1      |[0m 	enable.idempotence = false
[31morder_1      |[0m 	interceptor.classes = null
[31morder_1      |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[31morder_1      |[0m 	linger.ms = 0
[31morder_1      |[0m 	max.block.ms = 60000
[31morder_1      |[0m 	max.in.flight.requests.per.connection = 5
[31morder_1      |[0m 	max.request.size = 1048576
[31morder_1      |[0m 	metadata.max.age.ms = 300000
[31morder_1      |[0m 	metric.reporters = []
[31morder_1      |[0m 	metrics.num.samples = 2
[31morder_1      |[0m 	metrics.recording.level = INFO
[31morder_1      |[0m 	metrics.sample.window.ms = 30000
[31morder_1      |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[31morder_1      |[0m 	receive.buffer.bytes = 32768
[31morder_1      |[0m 	reconnect.backoff.max.ms = 1000
[31morder_1      |[0m 	reconnect.backoff.ms = 50
[31morder_1      |[0m 	request.timeout.ms = 30000
[31morder_1      |[0m 	retries = 0
[31morder_1      |[0m 	retry.backoff.ms = 100
[31morder_1      |[0m 	sasl.jaas.config = null
[31morder_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[31morder_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[31morder_1      |[0m 	sasl.kerberos.service.name = null
[31morder_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[31morder_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[31morder_1      |[0m 	sasl.mechanism = GSSAPI
[31morder_1      |[0m 	security.protocol = PLAINTEXT
[31morder_1      |[0m 	send.buffer.bytes = 131072
[31morder_1      |[0m 	ssl.cipher.suites = null
[31morder_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[31morder_1      |[0m 	ssl.endpoint.identification.algorithm = null
[31morder_1      |[0m 	ssl.key.password = null
[31morder_1      |[0m 	ssl.keymanager.algorithm = SunX509
[31morder_1      |[0m 	ssl.keystore.location = null
[31morder_1      |[0m 	ssl.keystore.password = null
[31morder_1      |[0m 	ssl.keystore.type = JKS
[31morder_1      |[0m 	ssl.protocol = TLS
[31morder_1      |[0m 	ssl.provider = null
[31morder_1      |[0m 	ssl.secure.random.implementation = null
[31morder_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[31morder_1      |[0m 	ssl.truststore.location = null
[31morder_1      |[0m 	ssl.truststore.password = null
[31morder_1      |[0m 	ssl.truststore.type = JKS
[31morder_1      |[0m 	transaction.timeout.ms = 60000
[31morder_1      |[0m 	transactional.id = null
[31morder_1      |[0m 	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer
[31morder_1      |[0m 
[31morder_1      |[0m 2018-12-30 04:19:16.528  INFO 6 --- [nio-8080-exec-5] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[31morder_1      |[0m 2018-12-30 04:19:16.535  INFO 6 --- [nio-8080-exec-5] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[32mkafka_1      |[0m [2018-12-30 04:19:17,476] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: order-1. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mshipping_1   |[0m 2018-12-30 04:19:18.532  INFO 6 --- [ntainer#0-0-C-1] c.e.m.s.events.OrderKafkaListener        : Revceived shipment 7
[34minvoicing_1  |[0m 2018-12-30 04:19:18.625  INFO 6 --- [ntainer#0-0-C-1] c.e.m.i.events.OrderKafkaListener        : Revceived invoice 7
[32mkafka_1      |[0m [2018-12-30 04:22:33,247] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:32:33,247] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:42:33,247] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 04:52:33,247] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34minvoicing_1  |[0m 2018-12-30 05:29:22.865  INFO 6 --- [ead | invoicing] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[34minvoicing_1  |[0m 2018-12-30 05:29:22.924  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[35mshipping_1   |[0m 2018-12-30 05:29:23.279  INFO 6 --- [read | shipping] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[33mzookeeper_1  |[0m 2018-12-30 05:29:23,313 [myid:] - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x167fd46c9780000, timeout of 6000ms exceeded
[33mzookeeper_1  |[0m 2018-12-30 05:29:23,315 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x167fd46c9780000
[33mzookeeper_1  |[0m 2018-12-30 05:29:23,320 [myid:] - INFO  [SyncThread:0:NIOServerCnxn@1007] - Closed socket connection for client /172.22.0.4:60460 which had sessionid 0x167fd46c9780000
[32mkafka_1      |[0m [2018-12-30 05:29:23,323] INFO Unable to read additional data from server sessionid 0x167fd46c9780000, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[35mshipping_1   |[0m 2018-12-30 05:29:23.348  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[32mkafka_1      |[0m [2018-12-30 05:29:23,426] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 05:29:23,428] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 05:29:24,837] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,837 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60706
[32mkafka_1      |[0m [2018-12-30 05:29:24,839] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,842 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@861] - Client attempting to renew session 0x167fd46c9780000 at /172.22.0.4:60706
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,843 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@610] - Invalid session 0x167fd46c9780000 for client /172.22.0.4:60706, probably expired
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,843 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /172.22.0.4:60706 which had sessionid 0x167fd46c9780000
[32mkafka_1      |[0m [2018-12-30 05:29:24,850] WARN Unable to reconnect to ZooKeeper service, session 0x167fd46c9780000 has expired (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 05:29:24,851] INFO Unable to reconnect to ZooKeeper service, session 0x167fd46c9780000 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 05:29:24,850] INFO EventThread shut down for session: 0x167fd46c9780000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 05:29:24,857] INFO [ZooKeeperClient] Session expired. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 05:29:24,964] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 05:29:24,964] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@223d2c72 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 05:29:24,973] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,974 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60708
[32mkafka_1      |[0m [2018-12-30 05:29:24,975] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,976 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.22.0.4:60708
[33mzookeeper_1  |[0m 2018-12-30 05:29:24,982 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x167fd46c9780002 with negotiated timeout 6000 for client /172.22.0.4:60708
[32mkafka_1      |[0m [2018-12-30 05:29:24,983] INFO Session establishment complete on server zookeeper/172.22.0.3:2181, sessionid = 0x167fd46c9780002, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 05:29:24,999] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 05:29:25,002] INFO Result of znode creation at /brokers/ids/1001 is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 05:29:25,003] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 05:29:25,006] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 05:29:25,009] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 05:29:25,148] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[33mzookeeper_1  |[0m 2018-12-30 05:29:25,162 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780002 type:delete cxid:0x4c zxid:0x9d txntype:-1 reqpath:n/a Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions
[33mzookeeper_1  |[0m 2018-12-30 05:29:25,169 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780002 type:delete cxid:0x4e zxid:0x9e txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[35mshipping_1   |[0m 2018-12-30 10:13:45.964  INFO 6 --- [read | shipping] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[34minvoicing_1  |[0m 2018-12-30 10:13:46.019  INFO 6 --- [ead | invoicing] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[35mshipping_1   |[0m 2018-12-30 10:13:46.070  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[34minvoicing_1  |[0m 2018-12-30 10:13:46.136  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,624 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,629 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,630 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,630 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,631 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,631 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,632 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,632 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,633 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,634 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,635 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 10:19:51,636 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[32mkafka_1      |[0m [2018-12-30 10:20:11,651] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 10:30:11,651] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 10:40:11,650] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-30 10:50:11,650] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[33mzookeeper_1  |[0m 2018-12-30 23:16:33,830 [myid:] - INFO  [SessionTracker:ZooKeeperServer@347] - Expiring session 0x167fd46c9780002, timeout of 6000ms exceeded
[35mshipping_1   |[0m 2018-12-30 23:16:35.079  INFO 6 --- [read | shipping] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[34minvoicing_1  |[0m 2018-12-30 23:16:35.090  INFO 6 --- [ead | invoicing] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Group coordinator kafka:9092 (id: 2147482646 rack: null) is unavailable or invalid, will attempt rediscovery
[33mzookeeper_1  |[0m 2018-12-30 23:16:35,188 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x167fd46c9780002
[33mzookeeper_1  |[0m 2018-12-30 23:16:35,191 [myid:] - INFO  [SyncThread:0:NIOServerCnxn@1007] - Closed socket connection for client /172.22.0.4:60708 which had sessionid 0x167fd46c9780002
[32mkafka_1      |[0m [2018-12-30 23:16:35,196] INFO Unable to read additional data from server sessionid 0x167fd46c9780002, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[34minvoicing_1  |[0m 2018-12-30 23:16:35.257  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[32mkafka_1      |[0m [2018-12-30 23:16:35,314] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 23:16:35,331] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[35mshipping_1   |[0m 2018-12-30 23:16:35.347  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,779 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60926
[32mkafka_1      |[0m [2018-12-30 23:16:36,777] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:36,781] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,783 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@861] - Client attempting to renew session 0x167fd46c9780002 at /172.22.0.4:60926
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,792 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@610] - Invalid session 0x167fd46c9780002 for client /172.22.0.4:60926, probably expired
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,793 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /172.22.0.4:60926 which had sessionid 0x167fd46c9780002
[32mkafka_1      |[0m [2018-12-30 23:16:36,794] INFO [ZooKeeperClient] Session expired. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 23:16:36,794] INFO EventThread shut down for session: 0x167fd46c9780002 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:36,800] WARN Unable to reconnect to ZooKeeper service, session 0x167fd46c9780002 has expired (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:36,801] INFO Unable to reconnect to ZooKeeper service, session 0x167fd46c9780002 has expired, closing socket connection (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:36,920] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-30 23:16:36,920] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@223d2c72 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-30 23:16:36,963] INFO Opening socket connection to server zookeeper/172.22.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,967 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.22.0.4:60928
[33mzookeeper_1  |[0m 2018-12-30 23:16:36,980 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.22.0.4:60928
[33mzookeeper_1  |[0m 2018-12-30 23:16:37,011 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x167fd46c9780003 with negotiated timeout 6000 for client /172.22.0.4:60928
[32mkafka_1      |[0m [2018-12-30 23:16:36,971] INFO Socket connection established to zookeeper/172.22.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:37,026] INFO Session establishment complete on server zookeeper/172.22.0.3:2181, sessionid = 0x167fd46c9780003, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-30 23:16:37,034] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 23:16:37,063] INFO Result of znode creation at /brokers/ids/1001 is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 23:16:37,064] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 23:16:37,086] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 23:16:37,102] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-30 23:16:37,630] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[33mzookeeper_1  |[0m 2018-12-30 23:16:37,666 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780003 type:delete cxid:0x4c zxid:0xa4 txntype:-1 reqpath:n/a Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions
[33mzookeeper_1  |[0m 2018-12-30 23:16:37,682 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x167fd46c9780003 type:delete cxid:0x4e zxid:0xa5 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[32mkafka_1      |[0m [2018-12-30 23:18:49,471] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,041 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,045 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,046 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,050 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,052 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,060 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,064 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,067 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,072 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,077 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,082 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,085 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,087 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,089 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,092 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,093 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,093 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,094 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,095 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,097 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,098 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,098 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,101 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,101 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,102 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-30 23:20:51,102 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[32mkafka_1      |[0m [2018-12-30 23:28:49,471] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
Gracefully stopping... (press Ctrl+C again to force)
