Attaching to mskafka_postgres_1, mskafka_zookeeper_1, mskafka_kafka_1, mskafka_shipping_1, mskafka_invoicing_1, mskafka_order_1, mskafka_apache_1
[36mpostgres_1   |[0m The files belonging to this database system will be owned by user "postgres".
[36mpostgres_1   |[0m This user must also own the server process.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m The database cluster will be initialized with locale "en_US.utf8".
[36mpostgres_1   |[0m The default database encoding has accordingly been set to "UTF8".
[36mpostgres_1   |[0m The default text search configuration will be set to "english".
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m Data page checksums are disabled.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m fixing permissions on existing directory /var/lib/postgresql/data ... ok
[36mpostgres_1   |[0m creating subdirectories ... ok
[36mpostgres_1   |[0m selecting default max_connections ... 100
[36mpostgres_1   |[0m selecting default shared_buffers ... 128MB
[36mpostgres_1   |[0m selecting dynamic shared memory implementation ... posix
[36mpostgres_1   |[0m creating configuration files ... ok
[33mzookeeper_1  |[0m JMX enabled by default
[33mzookeeper_1  |[0m Using config: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,187 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,251 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,269 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 1
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,272 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,327 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@138] - Purge task started.
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,599 [myid:] - INFO  [PurgeTask:DatadirCleanupManager$PurgeTask@144] - Purge task completed.
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,618 [myid:] - INFO  [main:QuorumPeerConfig@103] - Reading configuration from: /opt/zookeeper-3.4.6/bin/../conf/zoo.cfg
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,657 [myid:] - INFO  [main:ZooKeeperServerMain@95] - Starting server
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,836 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,837 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=978c38b8855a
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,837 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.7.0_65
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,837 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,837 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/lib/jvm/java-7-openjdk-amd64/jre
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,837 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/opt/zookeeper-3.4.6/bin/../build/classes:/opt/zookeeper-3.4.6/bin/../build/lib/*.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-log4j12-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/slf4j-api-1.6.1.jar:/opt/zookeeper-3.4.6/bin/../lib/netty-3.7.0.Final.jar:/opt/zookeeper-3.4.6/bin/../lib/log4j-1.2.16.jar:/opt/zookeeper-3.4.6/bin/../lib/jline-0.9.94.jar:/opt/zookeeper-3.4.6/bin/../zookeeper-3.4.6.jar:/opt/zookeeper-3.4.6/bin/../src/java/lib/*.jar:/opt/zookeeper-3.4.6/bin/../conf:
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,845 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,845 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp
[32mkafka_1      |[0m waiting for kafka to be ready
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,915 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,916 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux
[32mkafka_1      |[0m [Configuring] 'advertised.port' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m Excluding KAFKA_HOME from broker config
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,916 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,918 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=4.15.0-43-generic
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,918 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=root
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,918 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/root
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,918 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/opt/zookeeper-3.4.6
[32mkafka_1      |[0m [Configuring] 'advertised.host.name' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m [Configuring] 'port' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m [Configuring] 'broker.id' in '/opt/kafka/config/server.properties'
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,929 [myid:] - INFO  [main:ZooKeeperServer@755] - tickTime set to 2000
[32mkafka_1      |[0m Excluding KAFKA_VERSION from broker config
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,940 [myid:] - INFO  [main:ZooKeeperServer@764] - minSessionTimeout set to -1
[33mzookeeper_1  |[0m 2018-12-31 00:16:35,941 [myid:] - INFO  [main:ZooKeeperServer@773] - maxSessionTimeout set to -1
[32mkafka_1      |[0m [Configuring] 'zookeeper.connect' in '/opt/kafka/config/server.properties'
[32mkafka_1      |[0m [Configuring] 'log.dirs' in '/opt/kafka/config/server.properties'
[33mzookeeper_1  |[0m 2018-12-31 00:16:36,237 [myid:] - INFO  [main:NIOServerCnxnFactory@94] - binding to port 0.0.0.0/0.0.0.0:2181
[36;1mapache_1     |[0m [Mon Dec 31 00:16:41.359542 2018] [proxy_html:notice] [pid 8:tid 140089828489088] AH01425: I18n support in mod_proxy_html requires mod_xml2enc. Without it, non-ASCII characters in proxied pages are likely to display incorrectly.
[36;1mapache_1     |[0m AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 172.23.0.8. Set the 'ServerName' directive globally to suppress this message
[36mpostgres_1   |[0m running bootstrap script ... ok
[32mkafka_1      |[0m waiting for kafka to be ready
[32mkafka_1      |[0m [2018-12-31 00:16:44,510] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[36mpostgres_1   |[0m performing post-bootstrap initialization ... ok
[36mpostgres_1   |[0m syncing data to disk ... ok
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m WARNING: enabling "trust" authentication for local connections
[36mpostgres_1   |[0m You can change this by editing pg_hba.conf or using the option -A, or
[36mpostgres_1   |[0m --auth-local and --auth-host, the next time you run initdb.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m Success. You can now start the database server using:
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m     pg_ctl -D /var/lib/postgresql/data -l logfile start
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m waiting for server to start....LOG:  could not bind IPv6 socket: Cannot assign requested address
[36mpostgres_1   |[0m HINT:  Is another postmaster already running on port 5432? If not, wait a few seconds and retry.
[36mpostgres_1   |[0m LOG:  database system was shut down at 2018-12-31 00:16:45 UTC
[36mpostgres_1   |[0m LOG:  MultiXact member wraparound protections are now enabled
[36mpostgres_1   |[0m LOG:  database system is ready to accept connections
[36mpostgres_1   |[0m LOG:  autovacuum launcher started
[36mpostgres_1   |[0m  done
[36mpostgres_1   |[0m server started
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m CREATE ROLE
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init-user-db.sh
[32mkafka_1      |[0m [2018-12-31 00:16:48,974] INFO starting (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-31 00:16:49,000] INFO Connecting to zookeeper on zookeeper:2181 (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-31 00:16:49,136] INFO [ZooKeeperClient] Initializing a new session to zookeeper:2181. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-31 00:16:49,198] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,202] INFO Client environment:host.name=371fba1e3d68 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,204] INFO Client environment:java.version=1.8.0_171 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,206] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,207] INFO Client environment:java.home=/usr/lib/jvm/java-1.8-openjdk/jre (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,208] INFO Client environment:java.class.path=/opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.5.0-b42.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.5.0.jar:/opt/kafka/bin/../libs/commons-lang3-3.5.jar:/opt/kafka/bin/../libs/connect-api-2.0.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-2.0.0.jar:/opt/kafka/bin/../libs/connect-file-2.0.0.jar:/opt/kafka/bin/../libs/connect-json-2.0.0.jar:/opt/kafka/bin/../libs/connect-runtime-2.0.0.jar:/opt/kafka/bin/../libs/connect-transforms-2.0.0.jar:/opt/kafka/bin/../libs/guava-20.0.jar:/opt/kafka/bin/../libs/hk2-api-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-locator-2.5.0-b42.jar:/opt/kafka/bin/../libs/hk2-utils-2.5.0-b42.jar:/opt/kafka/bin/../libs/jackson-annotations-2.9.6.jar:/opt/kafka/bin/../libs/jackson-core-2.9.6.jar:/opt/kafka/bin/../libs/jackson-databind-2.9.6.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.9.6.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.9.6.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.9.6.jar:/opt/kafka/bin/../libs/javassist-3.22.0-CR2.jar:/opt/kafka/bin/../libs/javax.annotation-api-1.2.jar:/opt/kafka/bin/../libs/javax.inject-1.jar:/opt/kafka/bin/../libs/javax.inject-2.5.0-b42.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.0.jar:/opt/kafka/bin/../libs/jersey-client-2.27.jar:/opt/kafka/bin/../libs/jersey-common-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.27.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.27.jar:/opt/kafka/bin/../libs/jersey-hk2-2.27.jar:/opt/kafka/bin/../libs/jersey-media-jaxb-2.27.jar:/opt/kafka/bin/../libs/jersey-server-2.27.jar:/opt/kafka/bin/../libs/jetty-client-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-http-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-io-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-security-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-server-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jetty-util-9.4.11.v20180605.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/kafka-clients-2.0.0.jar:/opt/kafka/bin/../libs/kafka-log4j-appender-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.11-2.0.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-2.0.0.jar:/opt/kafka/bin/../libs/kafka-tools-2.0.0.jar:/opt/kafka/bin/../libs/kafka_2.11-2.0.0-sources.jar:/opt/kafka/bin/../libs/kafka_2.11-2.0.0.jar:/opt/kafka/bin/../libs/log4j-1.2.17.jar:/opt/kafka/bin/../libs/lz4-java-1.4.1.jar:/opt/kafka/bin/../libs/maven-artifact-3.5.3.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.1.jar:/opt/kafka/bin/../libs/plexus-utils-3.1.0.jar:/opt/kafka/bin/../libs/reflections-0.9.11.jar:/opt/kafka/bin/../libs/rocksdbjni-5.7.3.jar:/opt/kafka/bin/../libs/scala-library-2.11.12.jar:/opt/kafka/bin/../libs/scala-logging_2.11-3.9.0.jar:/opt/kafka/bin/../libs/scala-reflect-2.11.12.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.25.jar:/opt/kafka/bin/../libs/slf4j-log4j12-1.7.25.jar:/opt/kafka/bin/../libs/snappy-java-1.1.7.1.jar:/opt/kafka/bin/../libs/validation-api-1.1.0.Final.jar:/opt/kafka/bin/../libs/zkclient-0.10.jar:/opt/kafka/bin/../libs/zookeeper-3.4.13.jar (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,216] INFO Client environment:java.library.path=/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64/server:/usr/lib/jvm/java-1.8-openjdk/jre/lib/amd64:/usr/lib/jvm/java-1.8-openjdk/jre/../lib/amd64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,221] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,222] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,223] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,224] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,225] INFO Client environment:os.version=4.15.0-43-generic (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,228] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,228] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,238] INFO Client environment:user.dir=/ (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,248] INFO Initiating client connection, connectString=zookeeper:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@223d2c72 (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:16:49,389] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-31 00:16:49,422] INFO Opening socket connection to server zookeeper/172.23.0.3:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-31 00:16:49,469 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.23.0.4:43664
[32mkafka_1      |[0m [2018-12-31 00:16:49,481] INFO Socket connection established to zookeeper/172.23.0.3:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[33mzookeeper_1  |[0m 2018-12-31 00:16:49,527 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.23.0.4:43664
[33mzookeeper_1  |[0m 2018-12-31 00:16:49,548 [myid:] - INFO  [SyncThread:0:FileTxnLog@199] - Creating new log file: log.1
[32mkafka_1      |[0m [2018-12-31 00:16:49,615] INFO Session establishment complete on server zookeeper/172.23.0.3:2181, sessionid = 0x168019e942c0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[33mzookeeper_1  |[0m 2018-12-31 00:16:49,626 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x168019e942c0000 with negotiated timeout 6000 for client /172.23.0.4:43664
[32mkafka_1      |[0m [2018-12-31 00:16:49,665] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[33mzookeeper_1  |[0m 2018-12-31 00:16:50,359 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
[33mzookeeper_1  |[0m 2018-12-31 00:16:50,420 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
[33mzookeeper_1  |[0m 2018-12-31 00:16:50,482 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
[36mpostgres_1   |[0m CREATE DATABASE
[36mpostgres_1   |[0m GRANT
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m LOG:  received fast shutdown request
[36mpostgres_1   |[0m LOG:  aborting any active transactions
[36mpostgres_1   |[0m LOG:  autovacuum launcher shutting down
[36mpostgres_1   |[0m LOG:  shutting down
[36mpostgres_1   |[0m waiting for server to shut down....LOG:  database system is shut down
[36mpostgres_1   |[0m  done
[36mpostgres_1   |[0m server stopped
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m PostgreSQL init process complete; ready for start up.
[36mpostgres_1   |[0m 
[36mpostgres_1   |[0m LOG:  database system was shut down at 2018-12-31 00:16:51 UTC
[36mpostgres_1   |[0m LOG:  MultiXact member wraparound protections are now enabled
[36mpostgres_1   |[0m LOG:  database system is ready to accept connections
[36mpostgres_1   |[0m LOG:  autovacuum launcher started
[33mzookeeper_1  |[0m 2018-12-31 00:16:52,710 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
[32mkafka_1      |[0m [2018-12-31 00:16:52,797] INFO Cluster ID = 6nIOnI8fRpGs4VWR4vfMJA (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-31 00:16:52,862] WARN No meta.properties file under dir /kafka/kafka-logs-371fba1e3d68/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[32mkafka_1      |[0m waiting for kafka to be ready
[32mkafka_1      |[0m [2018-12-31 00:16:53,820] INFO KafkaConfig values: 
[32mkafka_1      |[0m 	advertised.host.name = kafka
[32mkafka_1      |[0m 	advertised.listeners = null
[32mkafka_1      |[0m 	advertised.port = 9092
[32mkafka_1      |[0m 	alter.config.policy.class.name = null
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	authorizer.class.name = 
[32mkafka_1      |[0m 	auto.create.topics.enable = true
[32mkafka_1      |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1      |[0m 	background.threads = 10
[32mkafka_1      |[0m 	broker.id = -1
[32mkafka_1      |[0m 	broker.id.generation.enable = true
[32mkafka_1      |[0m 	broker.rack = null
[32mkafka_1      |[0m 	client.quota.callback.class = null
[32mkafka_1      |[0m 	compression.type = producer
[32mkafka_1      |[0m 	connections.max.idle.ms = 600000
[32mkafka_1      |[0m 	controlled.shutdown.enable = true
[32mkafka_1      |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1      |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1      |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	create.topic.policy.class.name = null
[32mkafka_1      |[0m 	default.replication.factor = 1
[32mkafka_1      |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1      |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1      |[0m 	delegation.token.master.key = null
[32mkafka_1      |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1      |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1      |[0m 	delete.topic.enable = true
[32mkafka_1      |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1      |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1      |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1      |[0m 	host.name = 
[32mkafka_1      |[0m 	inter.broker.listener.name = null
[32mkafka_1      |[0m 	inter.broker.protocol.version = 2.0-IV1
[32mkafka_1      |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1      |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1      |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[32mkafka_1      |[0m 	listeners = null
[32mkafka_1      |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1      |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1      |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1      |[0m 	log.cleaner.enable = true
[32mkafka_1      |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1      |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1      |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1      |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1      |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1      |[0m 	log.cleaner.threads = 1
[32mkafka_1      |[0m 	log.cleanup.policy = [delete]
[32mkafka_1      |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1      |[0m 	log.dirs = /kafka/kafka-logs-371fba1e3d68
[32mkafka_1      |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.interval.ms = null
[32mkafka_1      |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.index.interval.bytes = 4096
[32mkafka_1      |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1      |[0m 	log.message.downconversion.enable = true
[32mkafka_1      |[0m 	log.message.format.version = 2.0-IV1
[32mkafka_1      |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1      |[0m 	log.preallocate = false
[32mkafka_1      |[0m 	log.retention.bytes = -1
[32mkafka_1      |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1      |[0m 	log.retention.hours = 168
[32mkafka_1      |[0m 	log.retention.minutes = null
[32mkafka_1      |[0m 	log.retention.ms = null
[32mkafka_1      |[0m 	log.roll.hours = 168
[32mkafka_1      |[0m 	log.roll.jitter.hours = 0
[32mkafka_1      |[0m 	log.roll.jitter.ms = null
[32mkafka_1      |[0m 	log.roll.ms = null
[32mkafka_1      |[0m 	log.segment.bytes = 1073741824
[32mkafka_1      |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1      |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1      |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1      |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1      |[0m 	message.max.bytes = 1000012
[32mkafka_1      |[0m 	metric.reporters = []
[32mkafka_1      |[0m 	metrics.num.samples = 2
[32mkafka_1      |[0m 	metrics.recording.level = INFO
[32mkafka_1      |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1      |[0m 	min.insync.replicas = 1
[32mkafka_1      |[0m 	num.io.threads = 8
[32mkafka_1      |[0m 	num.network.threads = 3
[32mkafka_1      |[0m 	num.partitions = 1
[32mkafka_1      |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1      |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1      |[0m 	num.replica.fetchers = 1
[32mkafka_1      |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1      |[0m 	offsets.commit.required.acks = -1
[32mkafka_1      |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1      |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1      |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1      |[0m 	offsets.retention.minutes = 10080
[32mkafka_1      |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1      |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1      |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1      |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1      |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1      |[0m 	password.encoder.iterations = 4096
[32mkafka_1      |[0m 	password.encoder.key.length = 128
[32mkafka_1      |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1      |[0m 	password.encoder.old.secret = null
[32mkafka_1      |[0m 	password.encoder.secret = null
[32mkafka_1      |[0m 	port = 9092
[32mkafka_1      |[0m 	principal.builder.class = null
[32mkafka_1      |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	queued.max.request.bytes = -1
[32mkafka_1      |[0m 	queued.max.requests = 500
[32mkafka_1      |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.window.num = 11
[32mkafka_1      |[0m 	quota.window.size.seconds = 1
[32mkafka_1      |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1      |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1      |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1      |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1      |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1      |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1      |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1      |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1      |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	replication.quota.window.num = 11
[32mkafka_1      |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	request.timeout.ms = 30000
[32mkafka_1      |[0m 	reserved.broker.max.id = 1000
[32mkafka_1      |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1      |[0m 	sasl.jaas.config = null
[32mkafka_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1      |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1      |[0m 	sasl.kerberos.service.name = null
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.login.class = null
[32mkafka_1      |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1      |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1      |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1      |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1      |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1      |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1      |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1      |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1      |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1      |[0m 	ssl.cipher.suites = []
[32mkafka_1      |[0m 	ssl.client.auth = none
[32mkafka_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1      |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1      |[0m 	ssl.key.password = null
[32mkafka_1      |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1      |[0m 	ssl.keystore.location = null
[32mkafka_1      |[0m 	ssl.keystore.password = null
[32mkafka_1      |[0m 	ssl.keystore.type = JKS
[32mkafka_1      |[0m 	ssl.protocol = TLS
[32mkafka_1      |[0m 	ssl.provider = null
[32mkafka_1      |[0m 	ssl.secure.random.implementation = null
[32mkafka_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1      |[0m 	ssl.truststore.location = null
[32mkafka_1      |[0m 	ssl.truststore.password = null
[32mkafka_1      |[0m 	ssl.truststore.type = JKS
[32mkafka_1      |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1      |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1      |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1      |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1      |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1      |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1      |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1      |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1      |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1      |[0m 	unclean.leader.election.enable = false
[32mkafka_1      |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1      |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1      |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.set.acl = false
[32mkafka_1      |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1      |[0m  (kafka.server.KafkaConfig)
[32mkafka_1      |[0m [2018-12-31 00:16:54,127] INFO KafkaConfig values: 
[32mkafka_1      |[0m 	advertised.host.name = kafka
[32mkafka_1      |[0m 	advertised.listeners = null
[32mkafka_1      |[0m 	advertised.port = 9092
[32mkafka_1      |[0m 	alter.config.policy.class.name = null
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.num = 11
[32mkafka_1      |[0m 	alter.log.dirs.replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	authorizer.class.name = 
[32mkafka_1      |[0m 	auto.create.topics.enable = true
[32mkafka_1      |[0m 	auto.leader.rebalance.enable = true
[32mkafka_1      |[0m 	background.threads = 10
[32mkafka_1      |[0m 	broker.id = -1
[32mkafka_1      |[0m 	broker.id.generation.enable = true
[32mkafka_1      |[0m 	broker.rack = null
[32mkafka_1      |[0m 	client.quota.callback.class = null
[32mkafka_1      |[0m 	compression.type = producer
[32mkafka_1      |[0m 	connections.max.idle.ms = 600000
[32mkafka_1      |[0m 	controlled.shutdown.enable = true
[32mkafka_1      |[0m 	controlled.shutdown.max.retries = 3
[32mkafka_1      |[0m 	controlled.shutdown.retry.backoff.ms = 5000
[32mkafka_1      |[0m 	controller.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	create.topic.policy.class.name = null
[32mkafka_1      |[0m 	default.replication.factor = 1
[32mkafka_1      |[0m 	delegation.token.expiry.check.interval.ms = 3600000
[32mkafka_1      |[0m 	delegation.token.expiry.time.ms = 86400000
[32mkafka_1      |[0m 	delegation.token.master.key = null
[32mkafka_1      |[0m 	delegation.token.max.lifetime.ms = 604800000
[32mkafka_1      |[0m 	delete.records.purgatory.purge.interval.requests = 1
[32mkafka_1      |[0m 	delete.topic.enable = true
[32mkafka_1      |[0m 	fetch.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	group.initial.rebalance.delay.ms = 0
[32mkafka_1      |[0m 	group.max.session.timeout.ms = 300000
[32mkafka_1      |[0m 	group.min.session.timeout.ms = 6000
[32mkafka_1      |[0m 	host.name = 
[32mkafka_1      |[0m 	inter.broker.listener.name = null
[32mkafka_1      |[0m 	inter.broker.protocol.version = 2.0-IV1
[32mkafka_1      |[0m 	leader.imbalance.check.interval.seconds = 300
[32mkafka_1      |[0m 	leader.imbalance.per.broker.percentage = 10
[32mkafka_1      |[0m 	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
[32mkafka_1      |[0m 	listeners = null
[32mkafka_1      |[0m 	log.cleaner.backoff.ms = 15000
[32mkafka_1      |[0m 	log.cleaner.dedupe.buffer.size = 134217728
[32mkafka_1      |[0m 	log.cleaner.delete.retention.ms = 86400000
[32mkafka_1      |[0m 	log.cleaner.enable = true
[32mkafka_1      |[0m 	log.cleaner.io.buffer.load.factor = 0.9
[32mkafka_1      |[0m 	log.cleaner.io.buffer.size = 524288
[32mkafka_1      |[0m 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
[32mkafka_1      |[0m 	log.cleaner.min.cleanable.ratio = 0.5
[32mkafka_1      |[0m 	log.cleaner.min.compaction.lag.ms = 0
[32mkafka_1      |[0m 	log.cleaner.threads = 1
[32mkafka_1      |[0m 	log.cleanup.policy = [delete]
[32mkafka_1      |[0m 	log.dir = /tmp/kafka-logs
[32mkafka_1      |[0m 	log.dirs = /kafka/kafka-logs-371fba1e3d68
[32mkafka_1      |[0m 	log.flush.interval.messages = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.interval.ms = null
[32mkafka_1      |[0m 	log.flush.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.flush.scheduler.interval.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.flush.start.offset.checkpoint.interval.ms = 60000
[32mkafka_1      |[0m 	log.index.interval.bytes = 4096
[32mkafka_1      |[0m 	log.index.size.max.bytes = 10485760
[32mkafka_1      |[0m 	log.message.downconversion.enable = true
[32mkafka_1      |[0m 	log.message.format.version = 2.0-IV1
[32mkafka_1      |[0m 	log.message.timestamp.difference.max.ms = 9223372036854775807
[32mkafka_1      |[0m 	log.message.timestamp.type = CreateTime
[32mkafka_1      |[0m 	log.preallocate = false
[32mkafka_1      |[0m 	log.retention.bytes = -1
[32mkafka_1      |[0m 	log.retention.check.interval.ms = 300000
[32mkafka_1      |[0m 	log.retention.hours = 168
[32mkafka_1      |[0m 	log.retention.minutes = null
[32mkafka_1      |[0m 	log.retention.ms = null
[32mkafka_1      |[0m 	log.roll.hours = 168
[32mkafka_1      |[0m 	log.roll.jitter.hours = 0
[32mkafka_1      |[0m 	log.roll.jitter.ms = null
[32mkafka_1      |[0m 	log.roll.ms = null
[32mkafka_1      |[0m 	log.segment.bytes = 1073741824
[32mkafka_1      |[0m 	log.segment.delete.delay.ms = 60000
[32mkafka_1      |[0m 	max.connections.per.ip = 2147483647
[32mkafka_1      |[0m 	max.connections.per.ip.overrides = 
[32mkafka_1      |[0m 	max.incremental.fetch.session.cache.slots = 1000
[32mkafka_1      |[0m 	message.max.bytes = 1000012
[32mkafka_1      |[0m 	metric.reporters = []
[32mkafka_1      |[0m 	metrics.num.samples = 2
[32mkafka_1      |[0m 	metrics.recording.level = INFO
[32mkafka_1      |[0m 	metrics.sample.window.ms = 30000
[32mkafka_1      |[0m 	min.insync.replicas = 1
[32mkafka_1      |[0m 	num.io.threads = 8
[32mkafka_1      |[0m 	num.network.threads = 3
[32mkafka_1      |[0m 	num.partitions = 1
[32mkafka_1      |[0m 	num.recovery.threads.per.data.dir = 1
[32mkafka_1      |[0m 	num.replica.alter.log.dirs.threads = null
[32mkafka_1      |[0m 	num.replica.fetchers = 1
[32mkafka_1      |[0m 	offset.metadata.max.bytes = 4096
[32mkafka_1      |[0m 	offsets.commit.required.acks = -1
[32mkafka_1      |[0m 	offsets.commit.timeout.ms = 5000
[32mkafka_1      |[0m 	offsets.load.buffer.size = 5242880
[32mkafka_1      |[0m 	offsets.retention.check.interval.ms = 600000
[32mkafka_1      |[0m 	offsets.retention.minutes = 10080
[32mkafka_1      |[0m 	offsets.topic.compression.codec = 0
[32mkafka_1      |[0m 	offsets.topic.num.partitions = 50
[32mkafka_1      |[0m 	offsets.topic.replication.factor = 1
[32mkafka_1      |[0m 	offsets.topic.segment.bytes = 104857600
[32mkafka_1      |[0m 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
[32mkafka_1      |[0m 	password.encoder.iterations = 4096
[32mkafka_1      |[0m 	password.encoder.key.length = 128
[32mkafka_1      |[0m 	password.encoder.keyfactory.algorithm = null
[32mkafka_1      |[0m 	password.encoder.old.secret = null
[32mkafka_1      |[0m 	password.encoder.secret = null
[32mkafka_1      |[0m 	port = 9092
[32mkafka_1      |[0m 	principal.builder.class = null
[32mkafka_1      |[0m 	producer.purgatory.purge.interval.requests = 1000
[32mkafka_1      |[0m 	queued.max.request.bytes = -1
[32mkafka_1      |[0m 	queued.max.requests = 500
[32mkafka_1      |[0m 	quota.consumer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.producer.default = 9223372036854775807
[32mkafka_1      |[0m 	quota.window.num = 11
[32mkafka_1      |[0m 	quota.window.size.seconds = 1
[32mkafka_1      |[0m 	replica.fetch.backoff.ms = 1000
[32mkafka_1      |[0m 	replica.fetch.max.bytes = 1048576
[32mkafka_1      |[0m 	replica.fetch.min.bytes = 1
[32mkafka_1      |[0m 	replica.fetch.response.max.bytes = 10485760
[32mkafka_1      |[0m 	replica.fetch.wait.max.ms = 500
[32mkafka_1      |[0m 	replica.high.watermark.checkpoint.interval.ms = 5000
[32mkafka_1      |[0m 	replica.lag.time.max.ms = 10000
[32mkafka_1      |[0m 	replica.socket.receive.buffer.bytes = 65536
[32mkafka_1      |[0m 	replica.socket.timeout.ms = 30000
[32mkafka_1      |[0m 	replication.quota.window.num = 11
[32mkafka_1      |[0m 	replication.quota.window.size.seconds = 1
[32mkafka_1      |[0m 	request.timeout.ms = 30000
[32mkafka_1      |[0m 	reserved.broker.max.id = 1000
[32mkafka_1      |[0m 	sasl.client.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.enabled.mechanisms = [GSSAPI]
[32mkafka_1      |[0m 	sasl.jaas.config = null
[32mkafka_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[32mkafka_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[32mkafka_1      |[0m 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
[32mkafka_1      |[0m 	sasl.kerberos.service.name = null
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[32mkafka_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.callback.handler.class = null
[32mkafka_1      |[0m 	sasl.login.class = null
[32mkafka_1      |[0m 	sasl.login.refresh.buffer.seconds = 300
[32mkafka_1      |[0m 	sasl.login.refresh.min.period.seconds = 60
[32mkafka_1      |[0m 	sasl.login.refresh.window.factor = 0.8
[32mkafka_1      |[0m 	sasl.login.refresh.window.jitter = 0.05
[32mkafka_1      |[0m 	sasl.mechanism.inter.broker.protocol = GSSAPI
[32mkafka_1      |[0m 	sasl.server.callback.handler.class = null
[32mkafka_1      |[0m 	security.inter.broker.protocol = PLAINTEXT
[32mkafka_1      |[0m 	socket.receive.buffer.bytes = 102400
[32mkafka_1      |[0m 	socket.request.max.bytes = 104857600
[32mkafka_1      |[0m 	socket.send.buffer.bytes = 102400
[32mkafka_1      |[0m 	ssl.cipher.suites = []
[32mkafka_1      |[0m 	ssl.client.auth = none
[32mkafka_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[32mkafka_1      |[0m 	ssl.endpoint.identification.algorithm = https
[32mkafka_1      |[0m 	ssl.key.password = null
[32mkafka_1      |[0m 	ssl.keymanager.algorithm = SunX509
[32mkafka_1      |[0m 	ssl.keystore.location = null
[32mkafka_1      |[0m 	ssl.keystore.password = null
[32mkafka_1      |[0m 	ssl.keystore.type = JKS
[32mkafka_1      |[0m 	ssl.protocol = TLS
[32mkafka_1      |[0m 	ssl.provider = null
[32mkafka_1      |[0m 	ssl.secure.random.implementation = null
[32mkafka_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[32mkafka_1      |[0m 	ssl.truststore.location = null
[32mkafka_1      |[0m 	ssl.truststore.password = null
[32mkafka_1      |[0m 	ssl.truststore.type = JKS
[32mkafka_1      |[0m 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
[32mkafka_1      |[0m 	transaction.max.timeout.ms = 900000
[32mkafka_1      |[0m 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
[32mkafka_1      |[0m 	transaction.state.log.load.buffer.size = 5242880
[32mkafka_1      |[0m 	transaction.state.log.min.isr = 1
[32mkafka_1      |[0m 	transaction.state.log.num.partitions = 50
[32mkafka_1      |[0m 	transaction.state.log.replication.factor = 1
[32mkafka_1      |[0m 	transaction.state.log.segment.bytes = 104857600
[32mkafka_1      |[0m 	transactional.id.expiration.ms = 604800000
[32mkafka_1      |[0m 	unclean.leader.election.enable = false
[32mkafka_1      |[0m 	zookeeper.connect = zookeeper:2181
[32mkafka_1      |[0m 	zookeeper.connection.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.max.in.flight.requests = 10
[32mkafka_1      |[0m 	zookeeper.session.timeout.ms = 6000
[32mkafka_1      |[0m 	zookeeper.set.acl = false
[32mkafka_1      |[0m 	zookeeper.sync.time.ms = 2000
[32mkafka_1      |[0m  (kafka.server.KafkaConfig)
[32mkafka_1      |[0m [2018-12-31 00:16:54,485] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:16:54,486] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:16:54,506] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:16:54,713] INFO Log directory /kafka/kafka-logs-371fba1e3d68 not found, creating it. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:16:54,794] INFO Loading logs. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:16:54,884] INFO Logs loading complete in 83 ms. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:16:55,025] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:16:55,039] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[35mshipping_1   |[0m 
[35mshipping_1   |[0m   .   ____          _            __ _ _
[35mshipping_1   |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[35mshipping_1   |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[35mshipping_1   |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[35mshipping_1   |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[35mshipping_1   |[0m  =========|_|==============|___/=/_/_/_/
[35mshipping_1   |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[35mshipping_1   |[0m 
[31minvoicing_1  |[0m 
[31minvoicing_1  |[0m   .   ____          _            __ _ _
[31minvoicing_1  |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[31minvoicing_1  |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[31minvoicing_1  |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[31minvoicing_1  |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[31minvoicing_1  |[0m  =========|_|==============|___/=/_/_/_/
[31minvoicing_1  |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[31minvoicing_1  |[0m 
[34morder_1      |[0m 
[34morder_1      |[0m   .   ____          _            __ _ _
[34morder_1      |[0m  /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
[34morder_1      |[0m ( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
[34morder_1      |[0m  \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
[34morder_1      |[0m   '  |____| .__|_| |_|_| |_\__, | / / / /
[34morder_1      |[0m  =========|_|==============|___/=/_/_/_/
[34morder_1      |[0m  :: Spring Boot ::        (v2.0.6.RELEASE)
[34morder_1      |[0m 
[32mkafka_1      |[0m [2018-12-31 00:16:59,750] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[35mshipping_1   |[0m 2018-12-31 00:17:00.166  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : Starting ShippingApp v0.0.1-SNAPSHOT on f540cca9b08f with PID 6 (/microservice-kafka-shipping-0.0.1-SNAPSHOT.jar started by root in /)
[35mshipping_1   |[0m 2018-12-31 00:17:00.256  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : No active profile set, falling back to default profiles: default
[32mkafka_1      |[0m [2018-12-31 00:17:00,274] INFO [SocketServer brokerId=1001] Started 1 acceptor threads (kafka.network.SocketServer)
[31minvoicing_1  |[0m 2018-12-31 00:17:00.321  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : Starting InvoiceApp v0.0.1-SNAPSHOT on d1428b6eaf2f with PID 6 (/microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar started by root in /)
[31minvoicing_1  |[0m 2018-12-31 00:17:00.381  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : No active profile set, falling back to default profiles: default
[32mkafka_1      |[0m [2018-12-31 00:17:00,621] INFO [ExpirationReaper-1001-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:00,622] INFO [ExpirationReaper-1001-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:00,744] INFO [ExpirationReaper-1001-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:01,036] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1      |[0m [2018-12-31 00:17:01,493] INFO Creating /brokers/ids/1001 (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-31 00:17:01,508] INFO Result of znode creation at /brokers/ids/1001 is: OK (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-31 00:17:01,535] INFO Registered broker 1001 at path /brokers/ids/1001 with addresses: ArrayBuffer(EndPoint(kafka,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-31 00:17:01,564] WARN No meta.properties file under dir /kafka/kafka-logs-371fba1e3d68/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[34morder_1      |[0m 2018-12-31 00:17:02.140  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : Starting OrderApp v0.0.1-SNAPSHOT on eceb696d35f2 with PID 6 (/microservice-kafka-order-0.0.1-SNAPSHOT.jar started by root in /)
[34morder_1      |[0m 2018-12-31 00:17:02.313  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : No active profile set, falling back to default profiles: default
[32mkafka_1      |[0m [2018-12-31 00:17:02,760] INFO [ExpirationReaper-1001-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:02,826] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[32mkafka_1      |[0m [2018-12-31 00:17:02,844] INFO [ExpirationReaper-1001-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:02,857] INFO [ExpirationReaper-1001-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:17:03,018] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[33mzookeeper_1  |[0m 2018-12-31 00:17:03,054 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:setData cxid:0x22 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
[32mkafka_1      |[0m [2018-12-31 00:17:03,333] INFO [GroupCoordinator 1001]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:17:03,357] INFO [GroupCoordinator 1001]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:17:03,420] INFO [GroupMetadataManager brokerId=1001] Removed 0 expired offsets in 48 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:17:03,546] INFO [ProducerId Manager 1001]: Acquired new producerId block (brokerId:1001,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[32mkafka_1      |[0m creating topics: order:5:1
[32mkafka_1      |[0m [2018-12-31 00:17:04,760] INFO [TransactionCoordinator id=1001] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:17:05,135] INFO [TransactionCoordinator id=1001] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:17:05,210] INFO [Transaction Marker Channel Manager 1001]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[33mzookeeper_1  |[0m 2018-12-31 00:17:07,554 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:delete cxid:0x32 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/admin/reassign_partitions Error:KeeperErrorCode = NoNode for /admin/reassign_partitions
[32mkafka_1      |[0m [2018-12-31 00:17:07,681] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1      |[0m [2018-12-31 00:17:07,735] INFO [SocketServer brokerId=1001] Started processors for 1 acceptors (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-31 00:17:07,770] INFO Kafka version : 2.0.0 (org.apache.kafka.common.utils.AppInfoParser)
[33mzookeeper_1  |[0m 2018-12-31 00:17:07,781 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:delete cxid:0x3a zxid:0x1f txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
[32mkafka_1      |[0m [2018-12-31 00:17:07,837] INFO Kafka commitId : 3402a8361b734732 (org.apache.kafka.common.utils.AppInfoParser)
[32mkafka_1      |[0m [2018-12-31 00:17:07,874] INFO [KafkaServer id=1001] started (kafka.server.KafkaServer)
[31minvoicing_1  |[0m 2018-12-31 00:17:08.293  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@18ef96: startup date [Mon Dec 31 00:17:08 GMT 2018]; root of context hierarchy
[35mshipping_1   |[0m 2018-12-31 00:17:08.688  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6956de9: startup date [Mon Dec 31 00:17:08 GMT 2018]; root of context hierarchy
[34morder_1      |[0m 2018-12-31 00:17:10.080  INFO 6 --- [           main] ConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@4f4a7090: startup date [Mon Dec 31 00:17:09 GMT 2018]; root of context hierarchy
[33mzookeeper_1  |[0m 2018-12-31 00:17:27,304 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /172.23.0.4:43734
[33mzookeeper_1  |[0m 2018-12-31 00:17:27,373 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@868] - Client attempting to establish new session at /172.23.0.4:43734
[33mzookeeper_1  |[0m 2018-12-31 00:17:27,381 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@617] - Established session 0x168019e942c0001 with negotiated timeout 30000 for client /172.23.0.4:43734
[33mzookeeper_1  |[0m 2018-12-31 00:17:30,732 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0001 type:setData cxid:0x4 zxid:0x21 txntype:-1 reqpath:n/a Error Path:/config/topics/order Error:KeeperErrorCode = NoNode for /config/topics/order
[32mkafka_1      |[0m Created topic "order".
[33mzookeeper_1  |[0m 2018-12-31 00:17:31,405 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x168019e942c0001
[33mzookeeper_1  |[0m 2018-12-31 00:17:31,410 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /172.23.0.4:43734 which had sessionid 0x168019e942c0001
[32mkafka_1      |[0m [2018-12-31 00:17:32,412] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions order-2,order-1,order-4,order-0,order-3 (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-31 00:17:32,822] INFO [Log partition=order-4, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:32,903] INFO [Log partition=order-4, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 356 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:32,919] INFO Created log for partition order-4 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:17:32,944] INFO [Partition order-4 broker=1001] No checkpointed highwatermark is found for partition order-4 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:32,997] INFO Replica loaded for partition order-4 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:17:33,000] INFO [Partition order-4 broker=1001] order-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,134] INFO [Log partition=order-1, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,156] INFO [Log partition=order-1, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 23 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,181] INFO Created log for partition order-1 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:17:33,182] INFO [Partition order-1 broker=1001] No checkpointed highwatermark is found for partition order-1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,183] INFO Replica loaded for partition order-1 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:17:33,185] INFO [Partition order-1 broker=1001] order-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,208] INFO [Log partition=order-2, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,239] INFO [Log partition=order-2, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,241] INFO Created log for partition order-2 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:17:33,255] INFO [Partition order-2 broker=1001] No checkpointed highwatermark is found for partition order-2 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,265] INFO Replica loaded for partition order-2 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:17:33,266] INFO [Partition order-2 broker=1001] order-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,280] INFO [Log partition=order-3, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,306] INFO [Log partition=order-3, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,347] INFO Created log for partition order-3 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:17:33,348] INFO [Partition order-3 broker=1001] No checkpointed highwatermark is found for partition order-3 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,348] INFO Replica loaded for partition order-3 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:17:33,350] INFO [Partition order-3 broker=1001] order-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,369] INFO [Log partition=order-0, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,403] INFO [Log partition=order-0, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 46 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:17:33,405] INFO Created log for partition order-0 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:17:33,412] INFO [Partition order-0 broker=1001] No checkpointed highwatermark is found for partition order-0 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,413] INFO Replica loaded for partition order-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:17:33,414] INFO [Partition order-0 broker=1001] order-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:17:33,487] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[31minvoicing_1  |[0m 2018-12-31 00:17:38.680  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$da52072b] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[35mshipping_1   |[0m 2018-12-31 00:17:39.117  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$c820ec42] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[31minvoicing_1  |[0m 2018-12-31 00:17:39.636  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$69ff5a8] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[35mshipping_1   |[0m 2018-12-31 00:17:40.044  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$f46edabf] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[34morder_1      |[0m 2018-12-31 00:17:41.724  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.kafka.annotation.KafkaBootstrapConfiguration' of type [org.springframework.kafka.annotation.KafkaBootstrapConfiguration$$EnhancerBySpringCGLIB$$cc2b266a] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[34morder_1      |[0m 2018-12-31 00:17:42.715  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$f87914e7] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[34morder_1      |[0m 2018-12-31 00:17:43.231  INFO 6 --- [           main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.hateoas.config.HateoasConfiguration' of type [org.springframework.hateoas.config.HateoasConfiguration$$EnhancerBySpringCGLIB$$77f96219] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
[31minvoicing_1  |[0m 2018-12-31 00:17:46.249  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[31minvoicing_1  |[0m 2018-12-31 00:17:46.773  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[31minvoicing_1  |[0m 2018-12-31 00:17:46.777  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[31minvoicing_1  |[0m 2018-12-31 00:17:46.937  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[35mshipping_1   |[0m 2018-12-31 00:17:46.978  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[35mshipping_1   |[0m 2018-12-31 00:17:47.441  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[35mshipping_1   |[0m 2018-12-31 00:17:47.455  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[35mshipping_1   |[0m 2018-12-31 00:17:47.589  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[31minvoicing_1  |[0m 2018-12-31 00:17:47.980  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[31minvoicing_1  |[0m 2018-12-31 00:17:47.985  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 39765 ms
[35mshipping_1   |[0m 2018-12-31 00:17:48.710  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[35mshipping_1   |[0m 2018-12-31 00:17:48.732  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 40056 ms
[34morder_1      |[0m 2018-12-31 00:17:49.748  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
[34morder_1      |[0m 2018-12-31 00:17:50.280  INFO 6 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
[34morder_1      |[0m 2018-12-31 00:17:50.307  INFO 6 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet Engine: Apache Tomcat/8.5.34
[34morder_1      |[0m 2018-12-31 00:17:50.485  INFO 6 --- [ost-startStop-1] o.a.catalina.core.AprLifecycleListener   : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib]
[34morder_1      |[0m 2018-12-31 00:17:51.835  INFO 6 --- [ost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring embedded WebApplicationContext
[34morder_1      |[0m 2018-12-31 00:17:51.856  INFO 6 --- [ost-startStop-1] o.s.web.context.ContextLoader            : Root WebApplicationContext: initialization completed in 41885 ms
[31minvoicing_1  |[0m 2018-12-31 00:17:56.530  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.549  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.552  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.552  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.553  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.554  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[31minvoicing_1  |[0m 2018-12-31 00:17:56.557  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[35mshipping_1   |[0m 2018-12-31 00:17:57.372  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.384  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.389  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.390  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.393  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.397  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[35mshipping_1   |[0m 2018-12-31 00:17:57.399  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[31minvoicing_1  |[0m 2018-12-31 00:17:57.801  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[35mshipping_1   |[0m 2018-12-31 00:17:58.667  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[31minvoicing_1  |[0m 2018-12-31 00:17:58.772  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[31minvoicing_1  |[0m 2018-12-31 00:17:59.474  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[31minvoicing_1  |[0m 2018-12-31 00:17:59.682  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[31minvoicing_1  |[0m 	name: default
[31minvoicing_1  |[0m 	...]
[35mshipping_1   |[0m 2018-12-31 00:17:59.920  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[35mshipping_1   |[0m 2018-12-31 00:18:00.688  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[31minvoicing_1  |[0m 2018-12-31 00:18:00.771  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[31minvoicing_1  |[0m 2018-12-31 00:18:00.775  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[35mshipping_1   |[0m 2018-12-31 00:18:00.831  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[35mshipping_1   |[0m 	name: default
[35mshipping_1   |[0m 	...]
[31minvoicing_1  |[0m 2018-12-31 00:18:01.411  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[34morder_1      |[0m 2018-12-31 00:18:01.566  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'characterEncodingFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.584  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'webMvcMetricsFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.588  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'hiddenHttpMethodFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.589  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpPutFormContentFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.595  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'requestContextFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.600  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.FilterRegistrationBean   : Mapping filter: 'httpTraceFilter' to: [/*]
[34morder_1      |[0m 2018-12-31 00:18:01.604  INFO 6 --- [ost-startStop-1] o.s.b.w.servlet.ServletRegistrationBean  : Servlet dispatcherServlet mapped to [/]
[35mshipping_1   |[0m 2018-12-31 00:18:01.749  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[35mshipping_1   |[0m 2018-12-31 00:18:01.775  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[35mshipping_1   |[0m 2018-12-31 00:18:02.271  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[34morder_1      |[0m 2018-12-31 00:18:02.674  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Starting...
[31minvoicing_1  |[0m 2018-12-31 00:18:03.325  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[34morder_1      |[0m 2018-12-31 00:18:03.835  INFO 6 --- [           main] com.zaxxer.hikari.HikariDataSource       : HikariPool-1 - Start completed.
[31minvoicing_1  |[0m 2018-12-31 00:18:04.292  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[31minvoicing_1  |[0m 
[31minvoicing_1  |[0m java.lang.reflect.InvocationTargetException: null
[31minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[31minvoicing_1  |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[31minvoicing_1  |[0m 	at com.ewolff.microservice.invoicing.InvoiceApp.main(InvoiceApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[31minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[31minvoicing_1  |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31minvoicing_1  |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31minvoicing_1  |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31minvoicing_1  |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-invoicing-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[31minvoicing_1  |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[31minvoicing_1  |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[31minvoicing_1  |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[31minvoicing_1  |[0m 	... 52 common frames omitted
[31minvoicing_1  |[0m 
[31minvoicing_1  |[0m 2018-12-31 00:18:04.327  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@22f31dec
[35mshipping_1   |[0m 2018-12-31 00:18:04.430  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[34morder_1      |[0m 2018-12-31 00:18:04.551  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default'
[34morder_1      |[0m 2018-12-31 00:18:04.748  INFO 6 --- [           main] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [
[34morder_1      |[0m 	name: default
[34morder_1      |[0m 	...]
[35mshipping_1   |[0m 2018-12-31 00:18:05.594  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[35mshipping_1   |[0m 
[35mshipping_1   |[0m java.lang.reflect.InvocationTargetException: null
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[35mshipping_1   |[0m 	at com.ewolff.microservice.shipping.ShippingApp.main(ShippingApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-shipping-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[35mshipping_1   |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[35mshipping_1   |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[35mshipping_1   |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[35mshipping_1   |[0m 	... 52 common frames omitted
[35mshipping_1   |[0m 
[35mshipping_1   |[0m 2018-12-31 00:18:05.626  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@34c01041
[34morder_1      |[0m 2018-12-31 00:18:05.877  INFO 6 --- [           main] org.hibernate.Version                    : HHH000412: Hibernate Core {5.2.17.Final}
[34morder_1      |[0m 2018-12-31 00:18:05.892  INFO 6 --- [           main] org.hibernate.cfg.Environment            : HHH000206: hibernate.properties not found
[34morder_1      |[0m 2018-12-31 00:18:06.319  INFO 6 --- [           main] o.hibernate.annotations.common.Version   : HCANN000001: Hibernate Commons Annotations {5.0.1.Final}
[34morder_1      |[0m 2018-12-31 00:18:08.288  INFO 6 --- [           main] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.PostgreSQL95Dialect
[31minvoicing_1  |[0m 2018-12-31 00:18:09.413  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[31minvoicing_1  |[0m 2018-12-31 00:18:09.420  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_nftn7w4ignckl0hkgot8ayrxm" of relation "invoice_invoice_line" does not exist, skipping
[31minvoicing_1  |[0m 2018-12-31 00:18:09.474  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[34morder_1      |[0m 2018-12-31 00:18:09.504  INFO 6 --- [           main] o.h.e.j.e.i.LobCreatorBuilderImpl        : HHH000424: Disabling contextual LOB creation as createClob() method threw error : java.lang.reflect.InvocationTargetException
[34morder_1      |[0m 
[34morder_1      |[0m java.lang.reflect.InvocationTargetException: null
[34morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[34morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[34morder_1      |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[34morder_1      |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.useContextualLobCreation(LobCreatorBuilderImpl.java:113) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.LobCreatorBuilderImpl.makeLobCreatorBuilder(LobCreatorBuilderImpl.java:54) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentImpl.<init>(JdbcEnvironmentImpl.java:271) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:114) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.env.internal.JdbcEnvironmentInitiator.initiateService(JdbcEnvironmentInitiator.java:35) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.initiateService(StandardServiceRegistryImpl.java:88) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.createService(AbstractServiceRegistryImpl.java:259) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:233) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.engine.jdbc.internal.JdbcServicesImpl.configure(JdbcServicesImpl.java:51) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.boot.registry.internal.StandardServiceRegistryImpl.configureService(StandardServiceRegistryImpl.java:94) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.initializeService(AbstractServiceRegistryImpl.java:242) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.service.internal.AbstractServiceRegistryImpl.getService(AbstractServiceRegistryImpl.java:210) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.handleTypes(MetadataBuildingProcess.java:352) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.boot.model.process.spi.MetadataBuildingProcess.complete(MetadataBuildingProcess.java:111) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.metadata(EntityManagerFactoryBuilderImpl.java:861) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.hibernate.jpa.boot.internal.EntityManagerFactoryBuilderImpl.build(EntityManagerFactoryBuilderImpl.java:888) [hibernate-core-5.2.17.Final.jar!/:5.2.17.Final]
[34morder_1      |[0m 	at org.springframework.orm.jpa.vendor.SpringHibernateJpaPersistenceProvider.createContainerEntityManagerFactory(SpringHibernateJpaPersistenceProvider.java:57) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.createNativeEntityManagerFactory(LocalContainerEntityManagerFactoryBean.java:365) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.buildNativeEntityManagerFactory(AbstractEntityManagerFactoryBean.java:390) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.orm.jpa.AbstractEntityManagerFactoryBean.afterPropertiesSet(AbstractEntityManagerFactoryBean.java:377) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean.afterPropertiesSet(LocalContainerEntityManagerFactoryBean.java:341) [spring-orm-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1753) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1690) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:573) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:495) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:317) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:315) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:199) [spring-beans-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:1087) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:857) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:548) ~[spring-context-5.0.10.RELEASE.jar!/:5.0.10.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:140) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:754) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:386) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1242) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1230) ~[spring-boot-2.0.6.RELEASE.jar!/:2.0.6.RELEASE]
[34morder_1      |[0m 	at com.ewolff.microservice.order.OrderApp.main(OrderApp.java:10) ~[classes!/:0.0.1-SNAPSHOT]
[34morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144]
[34morder_1      |[0m 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144]
[34morder_1      |[0m 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144]
[34morder_1      |[0m 	at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144]
[34morder_1      |[0m 	at org.springframework.boot.loader.MainMethodRunner.run(MainMethodRunner.java:48) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34morder_1      |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:87) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34morder_1      |[0m 	at org.springframework.boot.loader.Launcher.launch(Launcher.java:50) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34morder_1      |[0m 	at org.springframework.boot.loader.JarLauncher.main(JarLauncher.java:51) ~[microservice-kafka-order-0.0.1-SNAPSHOT.jar:0.0.1-SNAPSHOT]
[34morder_1      |[0m Caused by: java.sql.SQLFeatureNotSupportedException: Method org.postgresql.jdbc.PgConnection.createClob() is not yet implemented.
[34morder_1      |[0m 	at org.postgresql.Driver.notImplemented(Driver.java:688) ~[postgresql-42.2.5.jar!/:42.2.5]
[34morder_1      |[0m 	at org.postgresql.jdbc.PgConnection.createClob(PgConnection.java:1269) ~[postgresql-42.2.5.jar!/:42.2.5]
[34morder_1      |[0m 	... 52 common frames omitted
[34morder_1      |[0m 
[34morder_1      |[0m 2018-12-31 00:18:09.524  INFO 6 --- [           main] org.hibernate.type.BasicTypeRegistry     : HHH000270: Type registration [java.util.UUID] overrides previous : org.hibernate.type.UUIDBinaryType@5f9678e1
[35mshipping_1   |[0m 2018-12-31 00:18:11.886  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[35mshipping_1   |[0m 2018-12-31 00:18:11.900  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_bhctnoxmrnws4tpfowks2n5ip" of relation "shipment_shipment_line" does not exist, skipping
[35mshipping_1   |[0m 2018-12-31 00:18:11.943  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[31minvoicing_1  |[0m 2018-12-31 00:18:13.340  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[35mshipping_1   |[0m 2018-12-31 00:18:15.585  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[31minvoicing_1  |[0m 2018-12-31 00:18:16.394  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:16.590  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : SQL Warning Code: 0, SQLState: 00000
[34morder_1      |[0m 2018-12-31 00:18:16.597  WARN 6 --- [           main] o.h.engine.jdbc.spi.SqlExceptionHelper   : constraint "uk_1y2g3x9fjij1fjms7fa8tvkic" of relation "ordertable_order_line" does not exist, skipping
[34morder_1      |[0m 2018-12-31 00:18:16.658  INFO 6 --- [           main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
[35mshipping_1   |[0m 2018-12-31 00:18:18.331  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:19.897  INFO 6 --- [           main] o.h.h.i.QueryTranslatorFactoryInitiator  : HHH000397: Using ASTQueryTranslatorFactory
[31minvoicing_1  |[0m 2018-12-31 00:18:21.526  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@18ef96: startup date [Mon Dec 31 00:17:08 GMT 2018]; root of context hierarchy
[31minvoicing_1  |[0m 2018-12-31 00:18:22.121  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[31minvoicing_1  |[0m 2018-12-31 00:18:22.530  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.invoicing.web.InvoiceController.ItemList()
[31minvoicing_1  |[0m 2018-12-31 00:18:22.554  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.invoicing.web.InvoiceController.Item(long)
[31minvoicing_1  |[0m 2018-12-31 00:18:22.594  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[31minvoicing_1  |[0m 2018-12-31 00:18:22.616  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[35mshipping_1   |[0m 2018-12-31 00:18:23.062  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@6956de9: startup date [Mon Dec 31 00:17:08 GMT 2018]; root of context hierarchy
[31minvoicing_1  |[0m 2018-12-31 00:18:23.102  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[31minvoicing_1  |[0m 2018-12-31 00:18:23.121  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[35mshipping_1   |[0m 2018-12-31 00:18:23.622  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[35mshipping_1   |[0m 2018-12-31 00:18:24.080  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.shipping.web.ShippingController.ItemList()
[35mshipping_1   |[0m 2018-12-31 00:18:24.094  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.shipping.web.ShippingController.Item(long)
[35mshipping_1   |[0m 2018-12-31 00:18:24.144  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[35mshipping_1   |[0m 2018-12-31 00:18:24.148  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[35mshipping_1   |[0m 2018-12-31 00:18:24.563  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[35mshipping_1   |[0m 2018-12-31 00:18:24.570  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:30.968  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:33.661  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@4f4a7090: startup date [Mon Dec 31 00:17:09 GMT 2018]; root of context hierarchy
[31minvoicing_1  |[0m 2018-12-31 00:18:34.124  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[31minvoicing_1  |[0m 2018-12-31 00:18:34.255  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.270  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.288  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.289  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.297  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.301  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.304  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:34.303  WARN 6 --- [           main] aWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
[31minvoicing_1  |[0m 2018-12-31 00:18:34.308  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.328  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.342  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.346  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.348  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.366  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.374  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.380  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.393  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.424  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.426  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[31minvoicing_1  |[0m 2018-12-31 00:18:34.428  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[34morder_1      |[0m 2018-12-31 00:18:34.747  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/],methods=[POST]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.post(com.ewolff.microservice.order.logic.Order)
[34morder_1      |[0m 2018-12-31 00:18:34.771  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[DELETE]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.post(long)
[34morder_1      |[0m 2018-12-31 00:18:34.784  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.orderList()
[34morder_1      |[0m 2018-12-31 00:18:34.788  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/line],methods=[POST]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.addLine(com.ewolff.microservice.order.logic.Order)
[34morder_1      |[0m 2018-12-31 00:18:34.792  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/{id}],methods=[GET]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.get(long)
[34morder_1      |[0m 2018-12-31 00:18:34.793  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/form.html],methods=[GET]}" onto public org.springframework.web.servlet.ModelAndView com.ewolff.microservice.order.logic.OrderController.form()
[34morder_1      |[0m 2018-12-31 00:18:34.824  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[34morder_1      |[0m 2018-12-31 00:18:34.826  INFO 6 --- [           main] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest)
[31minvoicing_1  |[0m 2018-12-31 00:18:35.076  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[31minvoicing_1  |[0m 2018-12-31 00:18:35.105  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[31minvoicing_1  |[0m 2018-12-31 00:18:35.175  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[34morder_1      |[0m 2018-12-31 00:18:35.198  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:35.208  INFO 6 --- [           main] o.s.w.s.handler.SimpleUrlHandlerMapping  : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler]
[34morder_1      |[0m 2018-12-31 00:18:35.413  INFO 6 --- [           main] .m.m.a.ExceptionHandlerExceptionResolver : Detected @ExceptionHandler methods in repositoryRestExceptionHandler
[31minvoicing_1  |[0m 2018-12-31 00:18:35.515  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[35mshipping_1   |[0m 2018-12-31 00:18:35.716  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[31minvoicing_1  |[0m 2018-12-31 00:18:35.854  INFO 6 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
[31minvoicing_1  |[0m 	auto.commit.interval.ms = 5000
[31minvoicing_1  |[0m 	auto.offset.reset = earliest
[31minvoicing_1  |[0m 	bootstrap.servers = [kafka:9092]
[31minvoicing_1  |[0m 	check.crcs = true
[31minvoicing_1  |[0m 	client.id = 
[31minvoicing_1  |[0m 	connections.max.idle.ms = 540000
[31minvoicing_1  |[0m 	enable.auto.commit = false
[31minvoicing_1  |[0m 	exclude.internal.topics = true
[31minvoicing_1  |[0m 	fetch.max.bytes = 52428800
[31minvoicing_1  |[0m 	fetch.max.wait.ms = 500
[31minvoicing_1  |[0m 	fetch.min.bytes = 1
[31minvoicing_1  |[0m 	group.id = invoicing
[31minvoicing_1  |[0m 	heartbeat.interval.ms = 3000
[31minvoicing_1  |[0m 	interceptor.classes = null
[31minvoicing_1  |[0m 	internal.leave.group.on.close = true
[31minvoicing_1  |[0m 	isolation.level = read_uncommitted
[31minvoicing_1  |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[31minvoicing_1  |[0m 	max.partition.fetch.bytes = 1048576
[31minvoicing_1  |[0m 	max.poll.interval.ms = 300000
[31minvoicing_1  |[0m 	max.poll.records = 500
[31minvoicing_1  |[0m 	metadata.max.age.ms = 300000
[31minvoicing_1  |[0m 	metric.reporters = []
[31minvoicing_1  |[0m 	metrics.num.samples = 2
[31minvoicing_1  |[0m 	metrics.recording.level = INFO
[31minvoicing_1  |[0m 	metrics.sample.window.ms = 30000
[31minvoicing_1  |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[31minvoicing_1  |[0m 	receive.buffer.bytes = 65536
[31minvoicing_1  |[0m 	reconnect.backoff.max.ms = 1000
[31minvoicing_1  |[0m 	reconnect.backoff.ms = 50
[31minvoicing_1  |[0m 	request.timeout.ms = 305000
[31minvoicing_1  |[0m 	retry.backoff.ms = 100
[31minvoicing_1  |[0m 	sasl.jaas.config = null
[31minvoicing_1  |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[31minvoicing_1  |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[31minvoicing_1  |[0m 	sasl.kerberos.service.name = null
[31minvoicing_1  |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[31minvoicing_1  |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[31minvoicing_1  |[0m 	sasl.mechanism = GSSAPI
[31minvoicing_1  |[0m 	security.protocol = PLAINTEXT
[31minvoicing_1  |[0m 	send.buffer.bytes = 131072
[31minvoicing_1  |[0m 	session.timeout.ms = 10000
[31minvoicing_1  |[0m 	ssl.cipher.suites = null
[31minvoicing_1  |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[31minvoicing_1  |[0m 	ssl.endpoint.identification.algorithm = null
[31minvoicing_1  |[0m 	ssl.key.password = null
[31minvoicing_1  |[0m 	ssl.keymanager.algorithm = SunX509
[31minvoicing_1  |[0m 	ssl.keystore.location = null
[31minvoicing_1  |[0m 	ssl.keystore.password = null
[31minvoicing_1  |[0m 	ssl.keystore.type = JKS
[31minvoicing_1  |[0m 	ssl.protocol = TLS
[31minvoicing_1  |[0m 	ssl.provider = null
[31minvoicing_1  |[0m 	ssl.secure.random.implementation = null
[31minvoicing_1  |[0m 	ssl.trustmanager.algorithm = PKIX
[31minvoicing_1  |[0m 	ssl.truststore.location = null
[31minvoicing_1  |[0m 	ssl.truststore.password = null
[31minvoicing_1  |[0m 	ssl.truststore.type = JKS
[31minvoicing_1  |[0m 	value.deserializer = class com.ewolff.microservice.invoicing.events.InvoiceDeserializer
[31minvoicing_1  |[0m 
[35mshipping_1   |[0m 2018-12-31 00:18:35.893  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.897  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.912  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.918  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.923  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.928  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.939  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.940  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.944  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.948  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.964  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.983  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.984  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.986  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:35.998  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:36.001  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:36.011  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:36.015  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[35mshipping_1   |[0m 2018-12-31 00:18:36.027  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[35mshipping_1   |[0m 2018-12-31 00:18:36.780  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[35mshipping_1   |[0m 2018-12-31 00:18:36.800  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[35mshipping_1   |[0m 2018-12-31 00:18:36.902  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[31minvoicing_1  |[0m 2018-12-31 00:18:37.017  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[31minvoicing_1  |[0m 2018-12-31 00:18:37.032  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[31minvoicing_1  |[0m 2018-12-31 00:18:37.094  INFO 6 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
[35mshipping_1   |[0m 2018-12-31 00:18:37.207  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[35mshipping_1   |[0m 2018-12-31 00:18:37.527  INFO 6 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
[35mshipping_1   |[0m 	auto.commit.interval.ms = 5000
[35mshipping_1   |[0m 	auto.offset.reset = earliest
[35mshipping_1   |[0m 	bootstrap.servers = [kafka:9092]
[35mshipping_1   |[0m 	check.crcs = true
[35mshipping_1   |[0m 	client.id = 
[35mshipping_1   |[0m 	connections.max.idle.ms = 540000
[35mshipping_1   |[0m 	enable.auto.commit = false
[35mshipping_1   |[0m 	exclude.internal.topics = true
[35mshipping_1   |[0m 	fetch.max.bytes = 52428800
[35mshipping_1   |[0m 	fetch.max.wait.ms = 500
[35mshipping_1   |[0m 	fetch.min.bytes = 1
[35mshipping_1   |[0m 	group.id = shipping
[35mshipping_1   |[0m 	heartbeat.interval.ms = 3000
[35mshipping_1   |[0m 	interceptor.classes = null
[35mshipping_1   |[0m 	internal.leave.group.on.close = true
[35mshipping_1   |[0m 	isolation.level = read_uncommitted
[35mshipping_1   |[0m 	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
[35mshipping_1   |[0m 	max.partition.fetch.bytes = 1048576
[35mshipping_1   |[0m 	max.poll.interval.ms = 300000
[35mshipping_1   |[0m 	max.poll.records = 500
[35mshipping_1   |[0m 	metadata.max.age.ms = 300000
[35mshipping_1   |[0m 	metric.reporters = []
[35mshipping_1   |[0m 	metrics.num.samples = 2
[35mshipping_1   |[0m 	metrics.recording.level = INFO
[35mshipping_1   |[0m 	metrics.sample.window.ms = 30000
[35mshipping_1   |[0m 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
[35mshipping_1   |[0m 	receive.buffer.bytes = 65536
[35mshipping_1   |[0m 	reconnect.backoff.max.ms = 1000
[35mshipping_1   |[0m 	reconnect.backoff.ms = 50
[35mshipping_1   |[0m 	request.timeout.ms = 305000
[35mshipping_1   |[0m 	retry.backoff.ms = 100
[35mshipping_1   |[0m 	sasl.jaas.config = null
[35mshipping_1   |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[35mshipping_1   |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[35mshipping_1   |[0m 	sasl.kerberos.service.name = null
[35mshipping_1   |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[35mshipping_1   |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[35mshipping_1   |[0m 	sasl.mechanism = GSSAPI
[35mshipping_1   |[0m 	security.protocol = PLAINTEXT
[35mshipping_1   |[0m 	send.buffer.bytes = 131072
[35mshipping_1   |[0m 	session.timeout.ms = 10000
[35mshipping_1   |[0m 	ssl.cipher.suites = null
[35mshipping_1   |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[35mshipping_1   |[0m 	ssl.endpoint.identification.algorithm = null
[35mshipping_1   |[0m 	ssl.key.password = null
[35mshipping_1   |[0m 	ssl.keymanager.algorithm = SunX509
[35mshipping_1   |[0m 	ssl.keystore.location = null
[35mshipping_1   |[0m 	ssl.keystore.password = null
[35mshipping_1   |[0m 	ssl.keystore.type = JKS
[35mshipping_1   |[0m 	ssl.protocol = TLS
[35mshipping_1   |[0m 	ssl.provider = null
[35mshipping_1   |[0m 	ssl.secure.random.implementation = null
[35mshipping_1   |[0m 	ssl.trustmanager.algorithm = PKIX
[35mshipping_1   |[0m 	ssl.truststore.location = null
[35mshipping_1   |[0m 	ssl.truststore.password = null
[35mshipping_1   |[0m 	ssl.truststore.type = JKS
[35mshipping_1   |[0m 	value.deserializer = class com.ewolff.microservice.shipping.events.ShipmentDeserializer
[35mshipping_1   |[0m 
[31minvoicing_1  |[0m 2018-12-31 00:18:37.672  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[31minvoicing_1  |[0m 2018-12-31 00:18:37.716  INFO 6 --- [           main] c.e.microservice.invoicing.InvoiceApp    : Started InvoiceApp in 108.922 seconds (JVM running for 119.772)
[35mshipping_1   |[0m 2018-12-31 00:18:38.578  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[35mshipping_1   |[0m 2018-12-31 00:18:38.592  INFO 6 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[35mshipping_1   |[0m 2018-12-31 00:18:38.647  INFO 6 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
[33mzookeeper_1  |[0m 2018-12-31 00:18:38,940 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@645] - Got user-level KeeperException when processing sessionid:0x168019e942c0000 type:setData cxid:0x55 zxid:0x30 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets
[32mkafka_1      |[0m [2018-12-31 00:18:39,010] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(1001), __consumer_offsets-30 -> ArrayBuffer(1001), __consumer_offsets-8 -> ArrayBuffer(1001), __consumer_offsets-21 -> ArrayBuffer(1001), __consumer_offsets-4 -> ArrayBuffer(1001), __consumer_offsets-27 -> ArrayBuffer(1001), __consumer_offsets-7 -> ArrayBuffer(1001), __consumer_offsets-9 -> ArrayBuffer(1001), __consumer_offsets-46 -> ArrayBuffer(1001), __consumer_offsets-25 -> ArrayBuffer(1001), __consumer_offsets-35 -> ArrayBuffer(1001), __consumer_offsets-41 -> ArrayBuffer(1001), __consumer_offsets-33 -> ArrayBuffer(1001), __consumer_offsets-23 -> ArrayBuffer(1001), __consumer_offsets-49 -> ArrayBuffer(1001), __consumer_offsets-47 -> ArrayBuffer(1001), __consumer_offsets-16 -> ArrayBuffer(1001), __consumer_offsets-28 -> ArrayBuffer(1001), __consumer_offsets-31 -> ArrayBuffer(1001), __consumer_offsets-36 -> ArrayBuffer(1001), __consumer_offsets-42 -> ArrayBuffer(1001), __consumer_offsets-3 -> ArrayBuffer(1001), __consumer_offsets-18 -> ArrayBuffer(1001), __consumer_offsets-37 -> ArrayBuffer(1001), __consumer_offsets-15 -> ArrayBuffer(1001), __consumer_offsets-24 -> ArrayBuffer(1001), __consumer_offsets-38 -> ArrayBuffer(1001), __consumer_offsets-17 -> ArrayBuffer(1001), __consumer_offsets-48 -> ArrayBuffer(1001), __consumer_offsets-19 -> ArrayBuffer(1001), __consumer_offsets-11 -> ArrayBuffer(1001), __consumer_offsets-13 -> ArrayBuffer(1001), __consumer_offsets-2 -> ArrayBuffer(1001), __consumer_offsets-43 -> ArrayBuffer(1001), __consumer_offsets-6 -> ArrayBuffer(1001), __consumer_offsets-14 -> ArrayBuffer(1001), __consumer_offsets-20 -> ArrayBuffer(1001), __consumer_offsets-0 -> ArrayBuffer(1001), __consumer_offsets-44 -> ArrayBuffer(1001), __consumer_offsets-39 -> ArrayBuffer(1001), __consumer_offsets-12 -> ArrayBuffer(1001), __consumer_offsets-45 -> ArrayBuffer(1001), __consumer_offsets-1 -> ArrayBuffer(1001), __consumer_offsets-5 -> ArrayBuffer(1001), __consumer_offsets-26 -> ArrayBuffer(1001), __consumer_offsets-29 -> ArrayBuffer(1001), __consumer_offsets-34 -> ArrayBuffer(1001), __consumer_offsets-10 -> ArrayBuffer(1001), __consumer_offsets-32 -> ArrayBuffer(1001), __consumer_offsets-40 -> ArrayBuffer(1001)) (kafka.zk.AdminZkClient)
[32mkafka_1      |[0m [2018-12-31 00:18:39,123] INFO [KafkaApi-1001] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[35mshipping_1   |[0m 2018-12-31 00:18:39.424  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[35mshipping_1   |[0m 2018-12-31 00:18:39.525  INFO 6 --- [           main] c.e.microservice.shipping.ShippingApp    : Started ShippingApp in 111.229 seconds (JVM running for 122.252)
[32mkafka_1      |[0m [2018-12-31 00:18:41,730] INFO [ReplicaFetcherManager on broker 1001] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-31 00:18:41,839] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:41,864] INFO [Log partition=__consumer_offsets-0, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 101 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:41,868] INFO Created log for partition __consumer_offsets-0 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:41,878] INFO [Partition __consumer_offsets-0 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:41,885] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:41,888] INFO [Partition __consumer_offsets-0 broker=1001] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:41,981] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:41,984] INFO [Log partition=__consumer_offsets-29, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,023] INFO Created log for partition __consumer_offsets-29 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,026] INFO [Partition __consumer_offsets-29 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,061] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,061] INFO [Partition __consumer_offsets-29 broker=1001] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,095] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,118] INFO [Log partition=__consumer_offsets-48, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,192] INFO Created log for partition __consumer_offsets-48 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,194] INFO [Partition __consumer_offsets-48 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,194] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,195] INFO [Partition __consumer_offsets-48 broker=1001] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,273] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,292] INFO [Log partition=__consumer_offsets-10, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,308] INFO Created log for partition __consumer_offsets-10 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,309] INFO [Partition __consumer_offsets-10 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,321] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,322] INFO [Partition __consumer_offsets-10 broker=1001] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,403] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,413] INFO [Log partition=__consumer_offsets-45, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,425] INFO Created log for partition __consumer_offsets-45 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,435] INFO [Partition __consumer_offsets-45 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,447] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,448] INFO [Partition __consumer_offsets-45 broker=1001] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,499] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,523] INFO [Log partition=__consumer_offsets-26, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 25 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,553] INFO Created log for partition __consumer_offsets-26 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,605] INFO [Partition __consumer_offsets-26 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,609] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,623] INFO [Partition __consumer_offsets-26 broker=1001] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,701] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,709] INFO [Log partition=__consumer_offsets-7, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 47 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,714] INFO Created log for partition __consumer_offsets-7 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,765] INFO [Partition __consumer_offsets-7 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,765] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,766] INFO [Partition __consumer_offsets-7 broker=1001] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,771] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,820] INFO [Log partition=__consumer_offsets-42, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 50 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,823] INFO Created log for partition __consumer_offsets-42 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,836] INFO [Partition __consumer_offsets-42 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,860] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,861] INFO [Partition __consumer_offsets-42 broker=1001] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,927] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,940] INFO [Log partition=__consumer_offsets-4, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:42,952] INFO Created log for partition __consumer_offsets-4 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:42,972] INFO [Partition __consumer_offsets-4 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:42,982] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:42,987] INFO [Partition __consumer_offsets-4 broker=1001] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,026] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,061] INFO [Log partition=__consumer_offsets-23, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,085] INFO Created log for partition __consumer_offsets-23 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,108] INFO [Partition __consumer_offsets-23 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,109] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,114] INFO [Partition __consumer_offsets-23 broker=1001] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,123] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,193] INFO [Log partition=__consumer_offsets-1, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 72 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,202] INFO Created log for partition __consumer_offsets-1 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,212] INFO [Partition __consumer_offsets-1 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,213] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,214] INFO [Partition __consumer_offsets-1 broker=1001] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,280] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,295] INFO [Log partition=__consumer_offsets-20, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 28 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,310] INFO Created log for partition __consumer_offsets-20 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,313] INFO [Partition __consumer_offsets-20 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,325] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,326] INFO [Partition __consumer_offsets-20 broker=1001] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,372] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,386] INFO [Log partition=__consumer_offsets-39, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,428] INFO Created log for partition __consumer_offsets-39 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,438] INFO [Partition __consumer_offsets-39 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,440] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,441] INFO [Partition __consumer_offsets-39 broker=1001] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,501] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,506] INFO [Log partition=__consumer_offsets-17, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 31 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,516] INFO Created log for partition __consumer_offsets-17 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,545] INFO [Partition __consumer_offsets-17 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,550] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,584] INFO [Partition __consumer_offsets-17 broker=1001] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,609] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,613] INFO [Log partition=__consumer_offsets-36, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 20 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,660] INFO Created log for partition __consumer_offsets-36 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,686] INFO [Partition __consumer_offsets-36 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,700] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,701] INFO [Partition __consumer_offsets-36 broker=1001] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,750] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,760] INFO [Log partition=__consumer_offsets-14, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,799] INFO Created log for partition __consumer_offsets-14 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,801] INFO [Partition __consumer_offsets-14 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,823] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,824] INFO [Partition __consumer_offsets-14 broker=1001] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,834] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,916] INFO [Log partition=__consumer_offsets-33, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 83 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:43,918] INFO Created log for partition __consumer_offsets-33 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:43,934] INFO [Partition __consumer_offsets-33 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:43,934] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:43,941] INFO [Partition __consumer_offsets-33 broker=1001] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,002] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,012] INFO [Log partition=__consumer_offsets-49, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,019] INFO Created log for partition __consumer_offsets-49 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,037] INFO [Partition __consumer_offsets-49 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,045] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,046] INFO [Partition __consumer_offsets-49 broker=1001] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,137] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,156] INFO [Log partition=__consumer_offsets-11, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,158] INFO Created log for partition __consumer_offsets-11 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,170] INFO [Partition __consumer_offsets-11 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,180] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,180] INFO [Partition __consumer_offsets-11 broker=1001] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,250] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,260] INFO [Log partition=__consumer_offsets-30, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 32 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,263] INFO Created log for partition __consumer_offsets-30 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,278] INFO [Partition __consumer_offsets-30 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,297] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,327] INFO [Partition __consumer_offsets-30 broker=1001] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,360] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,374] INFO [Log partition=__consumer_offsets-46, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 36 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,393] INFO Created log for partition __consumer_offsets-46 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,439] INFO [Partition __consumer_offsets-46 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,448] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,448] INFO [Partition __consumer_offsets-46 broker=1001] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,482] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,486] INFO [Log partition=__consumer_offsets-27, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 29 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,499] INFO Created log for partition __consumer_offsets-27 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,562] INFO [Partition __consumer_offsets-27 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,576] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,577] INFO [Partition __consumer_offsets-27 broker=1001] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,594] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,612] INFO [Log partition=__consumer_offsets-8, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 19 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,637] INFO Created log for partition __consumer_offsets-8 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,665] INFO [Partition __consumer_offsets-8 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,688] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,689] INFO [Partition __consumer_offsets-8 broker=1001] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,713] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,757] INFO [Log partition=__consumer_offsets-24, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 58 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,781] INFO Created log for partition __consumer_offsets-24 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,786] INFO [Partition __consumer_offsets-24 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,805] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,806] INFO [Partition __consumer_offsets-24 broker=1001] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,820] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,896] INFO [Log partition=__consumer_offsets-43, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 77 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:44,912] INFO Created log for partition __consumer_offsets-43 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:44,965] INFO [Partition __consumer_offsets-43 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,966] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:44,966] INFO [Partition __consumer_offsets-43 broker=1001] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:44,975] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,023] INFO [Log partition=__consumer_offsets-5, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 49 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,028] INFO Created log for partition __consumer_offsets-5 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:45,039] INFO [Partition __consumer_offsets-5 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,073] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:45,074] INFO [Partition __consumer_offsets-5 broker=1001] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,114] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,124] INFO [Log partition=__consumer_offsets-21, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,144] INFO Created log for partition __consumer_offsets-21 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:45,197] INFO [Partition __consumer_offsets-21 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,200] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:45,201] INFO [Partition __consumer_offsets-21 broker=1001] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,628] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,641] INFO [Log partition=__consumer_offsets-2, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 420 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,665] INFO Created log for partition __consumer_offsets-2 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:45,674] INFO [Partition __consumer_offsets-2 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,689] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:45,690] INFO [Partition __consumer_offsets-2 broker=1001] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,761] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,766] INFO [Log partition=__consumer_offsets-40, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 57 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,784] INFO Created log for partition __consumer_offsets-40 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:45,788] INFO [Partition __consumer_offsets-40 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,796] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:45,797] INFO [Partition __consumer_offsets-40 broker=1001] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:45.801  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerAdapter   : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@4f4a7090: startup date [Mon Dec 31 00:17:09 GMT 2018]; root of context hierarchy
[32mkafka_1      |[0m [2018-12-31 00:18:45,866] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,869] INFO [Log partition=__consumer_offsets-37, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,879] INFO Created log for partition __consumer_offsets-37 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:45,888] INFO [Partition __consumer_offsets-37 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,905] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:45,906] INFO [Partition __consumer_offsets-37 broker=1001] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:45,930] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[34morder_1      |[0m 2018-12-31 00:18:45.940  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositoryController.optionsForRepositories()
[34morder_1      |[0m 2018-12-31 00:18:45.947  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryController.headForRepositories()
[34morder_1      |[0m 2018-12-31 00:18:45.947  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/ || ],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RepositoryLinksResource> org.springframework.data.rest.webmvc.RepositoryController.listRepositories()
[32mkafka_1      |[0m [2018-12-31 00:18:45,980] INFO [Log partition=__consumer_offsets-18, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 51 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:45,997] INFO Created log for partition __consumer_offsets-18 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[34morder_1      |[0m 2018-12-31 00:18:45.998  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation)
[32mkafka_1      |[0m [2018-12-31 00:18:46,003] INFO [Partition __consumer_offsets-18 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:46.004  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable) throws org.springframework.web.HttpRequestMethodNotSupportedException
[32mkafka_1      |[0m [2018-12-31 00:18:46,024] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,024] INFO [Partition __consumer_offsets-18 broker=1001] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:46.012  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[34morder_1      |[0m 2018-12-31 00:18:46.030  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[GET],produces=[application/x-spring-data-compact+json || text/uri-list]}" onto public org.springframework.hateoas.Resources<?> org.springframework.data.rest.webmvc.RepositoryEntityController.getCollectionResourceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[34morder_1      |[0m 2018-12-31 00:18:46.033  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}],methods=[POST],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.postCollectionResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException
[34morder_1      |[0m 2018-12-31 00:18:46.040  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.optionsForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation)
[34morder_1      |[0m 2018-12-31 00:18:46.046  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.headForItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws org.springframework.web.HttpRequestMethodNotSupportedException
[32mkafka_1      |[0m [2018-12-31 00:18:46,052] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[34morder_1      |[0m 2018-12-31 00:18:46.060  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.Resource<?>> org.springframework.data.rest.webmvc.RepositoryEntityController.getItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.http.HttpHeaders) throws org.springframework.web.HttpRequestMethodNotSupportedException
[32mkafka_1      |[0m [2018-12-31 00:18:46,070] INFO [Log partition=__consumer_offsets-34, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 33 ms (kafka.log.Log)
[34morder_1      |[0m 2018-12-31 00:18:46.071  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PUT],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.putItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.data.rest.webmvc.support.ETag,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException
[34morder_1      |[0m 2018-12-31 00:18:46.083  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[PATCH],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryEntityController.patchItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.data.rest.webmvc.PersistentEntityResource,java.io.Serializable,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.data.rest.webmvc.support.ETag,java.lang.String) throws org.springframework.web.HttpRequestMethodNotSupportedException,org.springframework.data.rest.webmvc.ResourceNotFoundException
[34morder_1      |[0m 2018-12-31 00:18:46.098  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositoryEntityController.deleteItemResource(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,org.springframework.data.rest.webmvc.support.ETag) throws org.springframework.data.rest.webmvc.ResourceNotFoundException,org.springframework.web.HttpRequestMethodNotSupportedException
[34morder_1      |[0m 2018-12-31 00:18:46.113  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[34morder_1      |[0m 2018-12-31 00:18:46.116  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[32mkafka_1      |[0m [2018-12-31 00:18:46,122] INFO Created log for partition __consumer_offsets-34 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[34morder_1      |[0m 2018-12-31 00:18:46.123  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String) throws java.lang.Exception
[32mkafka_1      |[0m [2018-12-31 00:18:46,130] INFO [Partition __consumer_offsets-34 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:46.132  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[GET],produces=[application/x-spring-data-compact+json || text/uri-list]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.followPropertyReferenceCompact(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler) throws java.lang.Exception
[34morder_1      |[0m 2018-12-31 00:18:46.141  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}],methods=[PATCH || PUT || POST],consumes=[application/json || application/x-spring-data-compact+json || text/uri-list],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<? extends org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.createPropertyReference(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.http.HttpMethod,org.springframework.hateoas.Resources<java.lang.Object>,java.io.Serializable,java.lang.String) throws java.lang.Exception
[34morder_1      |[0m 2018-12-31 00:18:46.143  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/{id}/{property}/{propertyId}],methods=[DELETE],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.RepositoryPropertyReferenceController.deletePropertyReferenceId(org.springframework.data.rest.webmvc.RootResourceInformation,java.io.Serializable,java.lang.String,java.lang.String) throws java.lang.Exception
[32mkafka_1      |[0m [2018-12-31 00:18:46,146] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,148] INFO [Partition __consumer_offsets-34 broker=1001] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:46.164  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[34morder_1      |[0m 2018-12-31 00:18:46.165  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[34morder_1      |[0m 2018-12-31 00:18:46.172  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.data.rest.webmvc.RepositorySearchesResource org.springframework.data.rest.webmvc.RepositorySearchController.listSearches(org.springframework.data.rest.webmvc.RootResourceInformation)
[34morder_1      |[0m 2018-12-31 00:18:46.178  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<?> org.springframework.data.rest.webmvc.RepositorySearchController.executeSearch(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.util.MultiValueMap<java.lang.String, java.lang.Object>,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler,org.springframework.http.HttpHeaders)
[34morder_1      |[0m 2018-12-31 00:18:46.195  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[GET],produces=[application/x-spring-data-compact+json]}" onto public org.springframework.hateoas.ResourceSupport org.springframework.data.rest.webmvc.RepositorySearchController.executeSearchCompact(org.springframework.data.rest.webmvc.RootResourceInformation,org.springframework.http.HttpHeaders,org.springframework.util.MultiValueMap<java.lang.String, java.lang.Object>,java.lang.String,java.lang.String,org.springframework.data.rest.webmvc.support.DefaultedPageable,org.springframework.data.domain.Sort,org.springframework.data.rest.webmvc.PersistentEntityResourceAssembler)
[34morder_1      |[0m 2018-12-31 00:18:46.197  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[OPTIONS],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.optionsForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
[34morder_1      |[0m 2018-12-31 00:18:46.200  INFO 6 --- [           main] o.s.d.r.w.RepositoryRestHandlerMapping   : Mapped "{[/{repository}/search/{search}],methods=[HEAD],produces=[application/hal+json || application/json]}" onto public org.springframework.http.ResponseEntity<java.lang.Object> org.springframework.data.rest.webmvc.RepositorySearchController.headForSearch(org.springframework.data.rest.webmvc.RootResourceInformation,java.lang.String)
[32mkafka_1      |[0m [2018-12-31 00:18:46,227] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[34morder_1      |[0m 2018-12-31 00:18:46.231  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile],methods=[OPTIONS]}" onto public org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.ProfileController.profileOptions()
[32mkafka_1      |[0m [2018-12-31 00:18:46,236] INFO [Log partition=__consumer_offsets-15, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 21 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,248] INFO Created log for partition __consumer_offsets-15 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[34morder_1      |[0m 2018-12-31 00:18:46.253  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile],methods=[GET]}" onto org.springframework.http.HttpEntity<org.springframework.hateoas.ResourceSupport> org.springframework.data.rest.webmvc.ProfileController.listAllFormsOfMetadata()
[32mkafka_1      |[0m [2018-12-31 00:18:46,256] INFO [Partition __consumer_offsets-15 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,257] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[34morder_1      |[0m 2018-12-31 00:18:46.262  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[GET],produces=[application/alps+json || */*]}" onto org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.RootResourceInformation> org.springframework.data.rest.webmvc.alps.AlpsController.descriptor(org.springframework.data.rest.webmvc.RootResourceInformation)
[34morder_1      |[0m 2018-12-31 00:18:46.263  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[OPTIONS],produces=[application/alps+json]}" onto org.springframework.http.HttpEntity<?> org.springframework.data.rest.webmvc.alps.AlpsController.alpsOptions()
[32mkafka_1      |[0m [2018-12-31 00:18:46,267] INFO [Partition __consumer_offsets-15 broker=1001] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[34morder_1      |[0m 2018-12-31 00:18:46.279  INFO 6 --- [           main] o.s.d.r.w.BasePathAwareHandlerMapping    : Mapped "{[/profile/{repository}],methods=[GET],produces=[application/schema+json]}" onto public org.springframework.http.HttpEntity<org.springframework.data.rest.webmvc.json.JsonSchema> org.springframework.data.rest.webmvc.RepositorySchemaController.schema(org.springframework.data.rest.webmvc.RootResourceInformation)
[32mkafka_1      |[0m [2018-12-31 00:18:46,348] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,353] INFO [Log partition=__consumer_offsets-12, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,393] INFO Created log for partition __consumer_offsets-12 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,399] INFO [Partition __consumer_offsets-12 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,401] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,402] INFO [Partition __consumer_offsets-12 broker=1001] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,442] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,455] INFO [Log partition=__consumer_offsets-31, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 45 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,472] INFO Created log for partition __consumer_offsets-31 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,474] INFO [Partition __consumer_offsets-31 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,494] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,497] INFO [Partition __consumer_offsets-31 broker=1001] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,515] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,522] INFO [Log partition=__consumer_offsets-9, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 8 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,560] INFO Created log for partition __consumer_offsets-9 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,563] INFO [Partition __consumer_offsets-9 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,564] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,565] INFO [Partition __consumer_offsets-9 broker=1001] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,580] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,618] INFO [Log partition=__consumer_offsets-47, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 39 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,625] INFO Created log for partition __consumer_offsets-47 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,627] INFO [Partition __consumer_offsets-47 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,652] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,653] INFO [Partition __consumer_offsets-47 broker=1001] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,666] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,672] INFO [Log partition=__consumer_offsets-19, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,721] INFO Created log for partition __consumer_offsets-19 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,731] INFO [Partition __consumer_offsets-19 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,732] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,733] INFO [Partition __consumer_offsets-19 broker=1001] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,742] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,776] INFO [Log partition=__consumer_offsets-28, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 35 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,785] INFO Created log for partition __consumer_offsets-28 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,787] INFO [Partition __consumer_offsets-28 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,798] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,799] INFO [Partition __consumer_offsets-28 broker=1001] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,822] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,843] INFO [Log partition=__consumer_offsets-38, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 37 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,847] INFO Created log for partition __consumer_offsets-38 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,868] INFO [Partition __consumer_offsets-38 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,873] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,873] INFO [Partition __consumer_offsets-38 broker=1001] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,879] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,904] INFO [Log partition=__consumer_offsets-35, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 26 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,905] INFO Created log for partition __consumer_offsets-35 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,907] INFO [Partition __consumer_offsets-35 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,907] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,908] INFO [Partition __consumer_offsets-35 broker=1001] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,947] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,980] INFO [Log partition=__consumer_offsets-44, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 30 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:46,982] INFO Created log for partition __consumer_offsets-44 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:46,983] INFO [Partition __consumer_offsets-44 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:46,984] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:46,984] INFO [Partition __consumer_offsets-44 broker=1001] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,011] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,016] INFO [Log partition=__consumer_offsets-6, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 16 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,023] INFO Created log for partition __consumer_offsets-6 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,037] INFO [Partition __consumer_offsets-6 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,084] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,087] INFO [Partition __consumer_offsets-6 broker=1001] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,104] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,107] INFO [Log partition=__consumer_offsets-25, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,112] INFO Created log for partition __consumer_offsets-25 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,118] INFO [Partition __consumer_offsets-25 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,119] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,120] INFO [Partition __consumer_offsets-25 broker=1001] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,162] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,164] INFO [Log partition=__consumer_offsets-16, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 34 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,189] INFO Created log for partition __consumer_offsets-16 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,193] INFO [Partition __consumer_offsets-16 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,196] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,196] INFO [Partition __consumer_offsets-16 broker=1001] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,217] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,220] INFO [Log partition=__consumer_offsets-22, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 18 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,222] INFO Created log for partition __consumer_offsets-22 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,233] INFO [Partition __consumer_offsets-22 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,234] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,236] INFO [Partition __consumer_offsets-22 broker=1001] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,262] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,272] INFO [Log partition=__consumer_offsets-41, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 11 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,299] INFO Created log for partition __consumer_offsets-41 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,310] INFO [Partition __consumer_offsets-41 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,310] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,311] INFO [Partition __consumer_offsets-41 broker=1001] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,323] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,332] INFO [Log partition=__consumer_offsets-32, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 10 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,340] INFO Created log for partition __consumer_offsets-32 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,343] INFO [Partition __consumer_offsets-32 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,368] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,368] INFO [Partition __consumer_offsets-32 broker=1001] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,374] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,397] INFO [Log partition=__consumer_offsets-3, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 24 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,401] INFO Created log for partition __consumer_offsets-3 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,411] INFO [Partition __consumer_offsets-3 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,413] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,414] INFO [Partition __consumer_offsets-3 broker=1001] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,436] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-371fba1e3d68] Loading producer state till offset 0 with message format version 2 (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,438] INFO [Log partition=__consumer_offsets-13, dir=/kafka/kafka-logs-371fba1e3d68] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[32mkafka_1      |[0m [2018-12-31 00:18:47,468] INFO Created log for partition __consumer_offsets-13 in /kafka/kafka-logs-371fba1e3d68 with properties {compression.type -> producer, message.format.version -> 2.0-IV1, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, message.downconversion.enable -> true, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,469] INFO [Partition __consumer_offsets-13 broker=1001] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,471] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[32mkafka_1      |[0m [2018-12-31 00:18:47,471] INFO [Partition __consumer_offsets-13 broker=1001] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[32mkafka_1      |[0m [2018-12-31 00:18:47,505] INFO [ReplicaAlterLogDirsManager on broker 1001] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,514] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,528] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,530] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,530] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,531] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,531] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,531] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,532] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,538] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,538] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,539] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,539] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,539] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,541] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,541] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,541] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,544] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,545] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,545] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,546] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,547] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,550] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,551] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,551] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,552] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,552] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,564] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,565] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,565] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,565] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,566] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,581] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,582] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,582] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,582] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,553] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-22 in 36 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,608] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,609] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,610] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-31 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,610] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,611] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-37 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,611] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,612] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,607] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,618] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,619] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,619] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,619] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,620] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,620] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,620] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,620] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,621] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,621] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,621] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,623] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,624] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,624] INFO [GroupMetadataManager brokerId=1001] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,624] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,626] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,628] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,632] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,637] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-47 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,655] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,677] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,678] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[31minvoicing_1  |[0m 2018-12-31 00:18:47.684  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[31minvoicing_1  |[0m 2018-12-31 00:18:47.693  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Revoking previously assigned partitions []
[31minvoicing_1  |[0m 2018-12-31 00:18:47.695  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions revoked: []
[31minvoicing_1  |[0m 2018-12-31 00:18:47.699  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] (Re-)joining group
[34morder_1      |[0m 2018-12-31 00:18:47.708  INFO 6 --- [           main] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 14 endpoint(s) beneath base path '/actuator'
[35mshipping_1   |[0m 2018-12-31 00:18:47.711  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Discovered group coordinator kafka:9092 (id: 2147482646 rack: null)
[32mkafka_1      |[0m [2018-12-31 00:18:47,722] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-10 in 33 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,726] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,727] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,728] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,728] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[35mshipping_1   |[0m 2018-12-31 00:18:47.725  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Revoking previously assigned partitions []
[35mshipping_1   |[0m 2018-12-31 00:18:47.732  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions revoked: []
[35mshipping_1   |[0m 2018-12-31 00:18:47.733  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] (Re-)joining group
[32mkafka_1      |[0m [2018-12-31 00:18:47,734] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,739] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,750] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,753] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,758] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,764] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,790] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,797] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,798] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,805] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,816] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,827] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,832] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,833] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-3 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,836] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-6 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,838] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-9 in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,853] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,854] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,859] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,872] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-21 in 11 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,873] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,874] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,875] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,876] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,877] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,881] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34morder_1      |[0m 2018-12-31 00:18:47.884  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/auditevents],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-31 00:18:47,885] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,889] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[32mkafka_1      |[0m [2018-12-31 00:18:47,889] INFO [GroupMetadataManager brokerId=1001] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[34morder_1      |[0m 2018-12-31 00:18:47.894  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/beans],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-31 00:18:47,892] INFO [GroupCoordinator 1001]: Preparing to rebalance group shipping with old generation 0 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:18:47,900] INFO [GroupCoordinator 1001]: Preparing to rebalance group invoicing with old generation 0 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[34morder_1      |[0m 2018-12-31 00:18:47.908  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/health],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.909  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/conditions],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.913  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/configprops],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.916  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env/{toMatch}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.917  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/env],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.918  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/info],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.922  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.923  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.930  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/loggers/{name}],methods=[POST],consumes=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.936  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/heapdump],methods=[GET],produces=[application/octet-stream]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.937  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/threaddump],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.938  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.938  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/metrics/{requiredMetricName}],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.950  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/scheduledtasks],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[32mkafka_1      |[0m [2018-12-31 00:18:47,954] INFO [GroupCoordinator 1001]: Stabilized group shipping generation 1 (__consumer_offsets-8) (kafka.coordinator.group.GroupCoordinator)
[34morder_1      |[0m 2018-12-31 00:18:47.955  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/httptrace],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.956  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator/mappings],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto public java.lang.Object org.springframework.boot.actuate.endpoint.web.servlet.AbstractWebMvcEndpointHandlerMapping$OperationHandler.handle(javax.servlet.http.HttpServletRequest,java.util.Map<java.lang.String, java.lang.String>)
[34morder_1      |[0m 2018-12-31 00:18:47.959  INFO 6 --- [           main] s.b.a.e.w.s.WebMvcEndpointHandlerMapping : Mapped "{[/actuator],methods=[GET],produces=[application/vnd.spring-boot.actuator.v2+json || application/json]}" onto protected java.util.Map<java.lang.String, java.util.Map<java.lang.String, org.springframework.boot.actuate.endpoint.web.Link>> org.springframework.boot.actuate.endpoint.web.servlet.WebMvcEndpointHandlerMapping.links(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)
[32mkafka_1      |[0m [2018-12-31 00:18:48,008] INFO [GroupCoordinator 1001]: Assignment received from leader for group shipping for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:18:48,017] INFO [GroupCoordinator 1001]: Stabilized group invoicing generation 1 (__consumer_offsets-10) (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:18:48,029] INFO [GroupCoordinator 1001]: Assignment received from leader for group invoicing for generation 1 (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:18:48,153] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-10. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[32mkafka_1      |[0m [2018-12-31 00:18:48,155] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-8. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[31minvoicing_1  |[0m 2018-12-31 00:18:48.273  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Successfully joined group with generation 1
[31minvoicing_1  |[0m 2018-12-31 00:18:48.276  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=invoicing] Setting newly assigned partitions [order-4, order-2, order-3, order-0, order-1]
[35mshipping_1   |[0m 2018-12-31 00:18:48.284  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Successfully joined group with generation 1
[35mshipping_1   |[0m 2018-12-31 00:18:48.288  INFO 6 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=shipping] Setting newly assigned partitions [order-4, order-2, order-3, order-0, order-1]
[34morder_1      |[0m 2018-12-31 00:18:48.482  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Registering beans for JMX exposure on startup
[34morder_1      |[0m 2018-12-31 00:18:48.497  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Bean with name 'dataSource' has been autodetected for JMX exposure
[35mshipping_1   |[0m 2018-12-31 00:18:48.500  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions assigned: [order-4, order-2, order-3, order-0, order-1]
[31minvoicing_1  |[0m 2018-12-31 00:18:48.508  INFO 6 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions assigned: [order-4, order-2, order-3, order-0, order-1]
[34morder_1      |[0m 2018-12-31 00:18:48.591  INFO 6 --- [           main] o.s.j.e.a.AnnotationMBeanExporter        : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource]
[34morder_1      |[0m 2018-12-31 00:18:48.660  INFO 6 --- [           main] o.s.c.support.DefaultLifecycleProcessor  : Starting beans in phase 2147483547
[34morder_1      |[0m 2018-12-31 00:18:48.872  INFO 6 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path ''
[34morder_1      |[0m 2018-12-31 00:18:48.880  INFO 6 --- [           main] com.ewolff.microservice.order.OrderApp   : Started OrderApp in 119.126 seconds (JVM running for 129.88)
[35mshipping_1   |[0m 2018-12-31 00:20:19.893  INFO 6 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
[35mshipping_1   |[0m 2018-12-31 00:20:19.894  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
[35mshipping_1   |[0m 2018-12-31 00:20:20.049  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 155 ms
[35mshipping_1   |[0m 2018-12-31 00:20:23.382  WARN 6 --- [nio-8080-exec-1] n.n.u.t.decorators.DecoratorProcessor    : The layout:decorator/data-layout-decorator processor has been deprecated and will be removed in the next major version of the layout dialect.  Please use layout:decorate/data-layout-decorate instead to future-proof your code.  See https://github.com/ultraq/thymeleaf-layout-dialect/issues/95 for more information.
[35mshipping_1   |[0m 2018-12-31 00:20:24.590  WARN 6 --- [nio-8080-exec-1] n.n.u.t.expressions.ExpressionProcessor  : Fragment expression "layout" is being wrapped as a Thymeleaf 3 fragment expression (~{...}) for backwards compatibility purposes.  This wrapping will be dropped in the next major version of the expression processor, so please rewrite as a Thymeleaf 3 fragment expression to future-proof your code.  See https://github.com/thymeleaf/thymeleaf/issues/451 for more information.
[34morder_1      |[0m 2018-12-31 00:20:31.344  INFO 6 --- [nio-8080-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/]       : Initializing Spring FrameworkServlet 'dispatcherServlet'
[34morder_1      |[0m 2018-12-31 00:20:31.345  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization started
[34morder_1      |[0m 2018-12-31 00:20:31.478  INFO 6 --- [nio-8080-exec-1] o.s.web.servlet.DispatcherServlet        : FrameworkServlet 'dispatcherServlet': initialization completed in 133 ms
[34morder_1      |[0m 2018-12-31 00:20:34.903  WARN 6 --- [nio-8080-exec-1] n.n.u.t.decorators.DecoratorProcessor    : The layout:decorator/data-layout-decorator processor has been deprecated and will be removed in the next major version of the layout dialect.  Please use layout:decorate/data-layout-decorate instead to future-proof your code.  See https://github.com/ultraq/thymeleaf-layout-dialect/issues/95 for more information.
[34morder_1      |[0m 2018-12-31 00:20:35.641  WARN 6 --- [nio-8080-exec-1] n.n.u.t.expressions.ExpressionProcessor  : Fragment expression "layout" is being wrapped as a Thymeleaf 3 fragment expression (~{...}) for backwards compatibility purposes.  This wrapping will be dropped in the next major version of the expression processor, so please rewrite as a Thymeleaf 3 fragment expression to future-proof your code.  See https://github.com/thymeleaf/thymeleaf/issues/451 for more information.
[34morder_1      |[0m 2018-12-31 00:21:07.337  INFO 6 --- [nio-8080-exec-9] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
[34morder_1      |[0m 	acks = 1
[34morder_1      |[0m 	batch.size = 16384
[34morder_1      |[0m 	bootstrap.servers = [kafka:9092]
[34morder_1      |[0m 	buffer.memory = 33554432
[34morder_1      |[0m 	client.id = 
[34morder_1      |[0m 	compression.type = none
[34morder_1      |[0m 	connections.max.idle.ms = 540000
[34morder_1      |[0m 	enable.idempotence = false
[34morder_1      |[0m 	interceptor.classes = null
[34morder_1      |[0m 	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
[34morder_1      |[0m 	linger.ms = 0
[34morder_1      |[0m 	max.block.ms = 60000
[34morder_1      |[0m 	max.in.flight.requests.per.connection = 5
[34morder_1      |[0m 	max.request.size = 1048576
[34morder_1      |[0m 	metadata.max.age.ms = 300000
[34morder_1      |[0m 	metric.reporters = []
[34morder_1      |[0m 	metrics.num.samples = 2
[34morder_1      |[0m 	metrics.recording.level = INFO
[34morder_1      |[0m 	metrics.sample.window.ms = 30000
[34morder_1      |[0m 	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
[34morder_1      |[0m 	receive.buffer.bytes = 32768
[34morder_1      |[0m 	reconnect.backoff.max.ms = 1000
[34morder_1      |[0m 	reconnect.backoff.ms = 50
[34morder_1      |[0m 	request.timeout.ms = 30000
[34morder_1      |[0m 	retries = 0
[34morder_1      |[0m 	retry.backoff.ms = 100
[34morder_1      |[0m 	sasl.jaas.config = null
[34morder_1      |[0m 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[34morder_1      |[0m 	sasl.kerberos.min.time.before.relogin = 60000
[34morder_1      |[0m 	sasl.kerberos.service.name = null
[34morder_1      |[0m 	sasl.kerberos.ticket.renew.jitter = 0.05
[34morder_1      |[0m 	sasl.kerberos.ticket.renew.window.factor = 0.8
[34morder_1      |[0m 	sasl.mechanism = GSSAPI
[34morder_1      |[0m 	security.protocol = PLAINTEXT
[34morder_1      |[0m 	send.buffer.bytes = 131072
[34morder_1      |[0m 	ssl.cipher.suites = null
[34morder_1      |[0m 	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
[34morder_1      |[0m 	ssl.endpoint.identification.algorithm = null
[34morder_1      |[0m 	ssl.key.password = null
[34morder_1      |[0m 	ssl.keymanager.algorithm = SunX509
[34morder_1      |[0m 	ssl.keystore.location = null
[34morder_1      |[0m 	ssl.keystore.password = null
[34morder_1      |[0m 	ssl.keystore.type = JKS
[34morder_1      |[0m 	ssl.protocol = TLS
[34morder_1      |[0m 	ssl.provider = null
[34morder_1      |[0m 	ssl.secure.random.implementation = null
[34morder_1      |[0m 	ssl.trustmanager.algorithm = PKIX
[34morder_1      |[0m 	ssl.truststore.location = null
[34morder_1      |[0m 	ssl.truststore.password = null
[34morder_1      |[0m 	ssl.truststore.type = JKS
[34morder_1      |[0m 	transaction.timeout.ms = 60000
[34morder_1      |[0m 	transactional.id = null
[34morder_1      |[0m 	value.serializer = class org.springframework.kafka.support.serializer.JsonSerializer
[34morder_1      |[0m 
[34morder_1      |[0m 2018-12-31 00:21:07.894  INFO 6 --- [nio-8080-exec-9] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 1.0.2
[34morder_1      |[0m 2018-12-31 00:21:07.900  INFO 6 --- [nio-8080-exec-9] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : 2a121f7b1d402825
[32mkafka_1      |[0m [2018-12-31 00:21:09,865] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: order-1. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[35mshipping_1   |[0m 2018-12-31 00:21:11.242  INFO 6 --- [ntainer#0-0-C-1] c.e.m.s.events.OrderKafkaListener        : Revceived shipment 7
[31minvoicing_1  |[0m 2018-12-31 00:21:11.316  INFO 6 --- [ntainer#0-0-C-1] c.e.m.i.events.OrderKafkaListener        : Revceived invoice 7
[36;1mmskafka_apache_1 exited with code 137
[0m[35mmskafka_shipping_1 exited with code 137
[0m[31mmskafka_invoicing_1 exited with code 137
[0m[34mmskafka_order_1 exited with code 137
[0m[36mpostgres_1   |[0m LOG:  received smart shutdown request
[36mpostgres_1   |[0m LOG:  autovacuum launcher shutting down
[36mpostgres_1   |[0m LOG:  shutting down
[32mkafka_1      |[0m [2018-12-31 00:24:50,078] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
[32mkafka_1      |[0m [2018-12-31 00:24:50,080] INFO [KafkaServer id=1001] shutting down (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-31 00:24:50,093] INFO [KafkaServer id=1001] Starting controlled shutdown (kafka.server.KafkaServer)
[36mpostgres_1   |[0m LOG:  database system is shut down
[32mkafka_1      |[0m [2018-12-31 00:24:50,219] INFO [KafkaServer id=1001] Controlled shutdown succeeded (kafka.server.KafkaServer)
[32mkafka_1      |[0m [2018-12-31 00:24:50,282] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1      |[0m [2018-12-31 00:24:50,288] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1      |[0m [2018-12-31 00:24:50,289] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[32mkafka_1      |[0m [2018-12-31 00:24:50,290] INFO [SocketServer brokerId=1001] Stopping socket server request processors (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-31 00:24:50,338] INFO [SocketServer brokerId=1001] Stopped socket server request processors (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-31 00:24:50,345] INFO [Kafka Request Handler on Broker 1001], shutting down (kafka.server.KafkaRequestHandlerPool)
[32mkafka_1      |[0m [2018-12-31 00:24:50,359] INFO [Kafka Request Handler on Broker 1001], shut down completely (kafka.server.KafkaRequestHandlerPool)
[32mkafka_1      |[0m [2018-12-31 00:24:50,376] INFO [KafkaApi-1001] Shutdown complete. (kafka.server.KafkaApis)
[32mkafka_1      |[0m [2018-12-31 00:24:50,378] INFO [ExpirationReaper-1001-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,429] INFO [ExpirationReaper-1001-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,430] INFO [ExpirationReaper-1001-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,435] INFO [TransactionCoordinator id=1001] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:24:50,439] INFO [ProducerId Manager 1001]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,439] INFO [Transaction State Manager 1001]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,439] INFO [Transaction Marker Channel Manager 1001]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,440] INFO [Transaction Marker Channel Manager 1001]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,443] INFO [Transaction Marker Channel Manager 1001]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,444] INFO [TransactionCoordinator id=1001] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:24:50,444] INFO [GroupCoordinator 1001]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:24:50,447] INFO [ExpirationReaper-1001-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,605] INFO [ExpirationReaper-1001-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,606] INFO [ExpirationReaper-1001-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,606] INFO [ExpirationReaper-1001-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,635] INFO [ExpirationReaper-1001-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,635] INFO [ExpirationReaper-1001-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,636] INFO [GroupCoordinator 1001]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[32mkafka_1      |[0m [2018-12-31 00:24:50,637] INFO [ReplicaManager broker=1001] Shutting down (kafka.server.ReplicaManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,637] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1      |[0m [2018-12-31 00:24:50,637] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1      |[0m [2018-12-31 00:24:50,639] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[32mkafka_1      |[0m [2018-12-31 00:24:50,640] INFO [ReplicaFetcherManager on broker 1001] shutting down (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,651] INFO [ReplicaFetcherManager on broker 1001] shutdown completed (kafka.server.ReplicaFetcherManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,652] INFO [ReplicaAlterLogDirsManager on broker 1001] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,652] INFO [ReplicaAlterLogDirsManager on broker 1001] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,652] INFO [ExpirationReaper-1001-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,658] INFO [ExpirationReaper-1001-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,660] INFO [ExpirationReaper-1001-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,660] INFO [ExpirationReaper-1001-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[36mmskafka_postgres_1 exited with code 0
[0m[32mkafka_1      |[0m [2018-12-31 00:24:50,833] INFO [ExpirationReaper-1001-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,833] INFO [ExpirationReaper-1001-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,833] INFO [ExpirationReaper-1001-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,836] INFO [ExpirationReaper-1001-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,836] INFO [ExpirationReaper-1001-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:50,845] INFO [ReplicaManager broker=1001] Shut down completely (kafka.server.ReplicaManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,846] INFO Shutting down. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,903] INFO [ProducerStateManager partition=__consumer_offsets-8] Writing producer snapshot at offset 7 (kafka.log.ProducerStateManager)
[32mkafka_1      |[0m [2018-12-31 00:24:50,928] INFO [ProducerStateManager partition=order-1] Writing producer snapshot at offset 1 (kafka.log.ProducerStateManager)
[32mkafka_1      |[0m [2018-12-31 00:24:51,018] INFO [ProducerStateManager partition=__consumer_offsets-10] Writing producer snapshot at offset 7 (kafka.log.ProducerStateManager)
[32mkafka_1      |[0m [2018-12-31 00:24:51,055] INFO Shutdown complete. (kafka.log.LogManager)
[32mkafka_1      |[0m [2018-12-31 00:24:51,072] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[33mzookeeper_1  |[0m 2018-12-31 00:24:51,074 [myid:] - INFO  [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@494] - Processed session termination for sessionid: 0x168019e942c0000
[33mzookeeper_1  |[0m 2018-12-31 00:24:51,078 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /172.23.0.4:43664 which had sessionid 0x168019e942c0000
[32mkafka_1      |[0m [2018-12-31 00:24:51,087] INFO EventThread shut down for session: 0x168019e942c0000 (org.apache.zookeeper.ClientCnxn)
[32mkafka_1      |[0m [2018-12-31 00:24:51,089] INFO Session: 0x168019e942c0000 closed (org.apache.zookeeper.ZooKeeper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,091] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[32mkafka_1      |[0m [2018-12-31 00:24:51,093] INFO [ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,126] INFO [ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,127] INFO [ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,128] INFO [ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,184] INFO [ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,188] INFO [ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:51,189] INFO [ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:52,186] INFO [ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:52,187] INFO [ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
[32mkafka_1      |[0m [2018-12-31 00:24:52,191] INFO [SocketServer brokerId=1001] Shutting down socket server (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-31 00:24:52,330] INFO [SocketServer brokerId=1001] Shutdown completed (kafka.network.SocketServer)
[32mkafka_1      |[0m [2018-12-31 00:24:52,353] INFO [KafkaServer id=1001] shut down completed (kafka.server.KafkaServer)
[32mmskafka_kafka_1 exited with code 143
[0m[33mmskafka_zookeeper_1 exited with code 137
[0m